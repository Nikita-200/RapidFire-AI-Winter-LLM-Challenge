{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc8dMxjZ3xCO"
      },
      "source": [
        "Install & setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeOJRDEll5pR",
        "outputId": "f257d882-c2f8-4338-de78-fbfbae47ae19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rapidfireai installed\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import rapidfireai\n",
        "    print(\"rapidfireai installed\")\n",
        "except ImportError:\n",
        "    !pip install rapidfireai datasets==3.6.0 langchain sentence-transformers\n",
        "    !rapidfireai init --evals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY6TrfRa31-E"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uyPBQTeFkNEw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List as listtype, Dict, Any\n",
        "from datasets import load_dataset\n",
        "\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B2Pho4r36Zd"
      },
      "source": [
        "Load & prepare dataset, Build corpus + queries + qrels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "# Setup\n",
        "dataset_dir = Path(\"./electronics_rag\")\n",
        "dataset_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Load and Filter\n",
        "raw_dataset = load_dataset(\"buruzaemon/amazon_reviews_multi\", \"en\", split=\"train\")\n",
        "electronics_data = raw_dataset.filter(lambda x: \"electronics\" in x[\"product_category\"].lower())\n",
        "\n",
        "# Downsample (Using a larger set to ensure product overlaps)\n",
        "sample_size = 100\n",
        "rseed = 42\n",
        "random.seed(rseed)\n",
        "sampled_data = electronics_data.shuffle(seed=rseed).select(range(sample_size))\n",
        "\n",
        "# Grouping Logic to know which documents belong to which product\n",
        "product_to_docs = defaultdict(list)\n",
        "corpus_list = []\n",
        "queries_list = []\n",
        "\n",
        "for i, row in enumerate(sampled_data):\n",
        "    doc_id = f\"doc_{i}\"\n",
        "    query_id = f\"q_{i}\"\n",
        "    prod_id = str(row['product_id'])\n",
        "\n",
        "    # Store the document\n",
        "    corpus_list.append({\"_id\": doc_id, \"text\": row[\"review_body\"]})\n",
        "\n",
        "    # Store the query (using title)\n",
        "    queries_list.append({\"query_id\": query_id, \"query\": row[\"review_title\"]})\n",
        "\n",
        "    # Map this document to its product group\n",
        "    product_to_docs[prod_id].append(doc_id)\n",
        "\n",
        "# Build Expanded QRELS\n",
        "qrels_rows = []\n",
        "for i, row in enumerate(sampled_data):\n",
        "    query_id = f\"q_{i}\"\n",
        "    prod_id = str(row['product_id'])\n",
        "\n",
        "    # Every document sharing this product_id is now a \"correct\" answer\n",
        "    relevant_docs = product_to_docs[prod_id]\n",
        "\n",
        "    for d_id in relevant_docs:\n",
        "        qrels_rows.append({\n",
        "            \"query_id\": query_id,\n",
        "            \"corpus_id\": d_id,\n",
        "            \"relevance\": 1\n",
        "        })\n",
        "\n",
        "# Save and Finalize\n",
        "corpus_file = dataset_dir / \"corpus_sampled.jsonl\"\n",
        "with open(corpus_file, \"w\") as f:\n",
        "    for doc in corpus_list:\n",
        "        f.write(json.dumps(doc) + \"\\n\")\n",
        "\n",
        "electronics_dataset = pd.DataFrame(queries_list).astype(str)\n",
        "qrels = pd.DataFrame(qrels_rows).astype(str)\n",
        "\n",
        "print(f\"Prepared {len(corpus_list)} documents.\")\n",
        "print(f\"Expanded QRELS: {len(qrels)} relevance pairs (Multiple reviews per product).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_4xxB7s_kl-",
        "outputId": "8affe0ea-265d-4edd-e8a4-2283df1f6444"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared 100 documents.\n",
            "Expanded QRELS: 100 relevance pairs (Multiple reviews per product).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDvR6QKY4Otx"
      },
      "source": [
        "Define RAG search space (retrieval-focused)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
        "\n",
        "# Batch size for embedding model hardware efficiency\n",
        "batch_size = 50\n",
        "\n",
        "rag_gpu = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        path=str(dataset_dir),\n",
        "        glob=\"corpus_sampled.jsonl\",\n",
        "        loader_cls=JSONLoader,\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"content_key\": \"text\",\n",
        "            \"metadata_func\": lambda record, metadata: {\n",
        "                \"corpus_id\": str(record.get(\"_id\"))\n",
        "            },\n",
        "            \"json_lines\": True,\n",
        "            \"text_content\": False,\n",
        "        },\n",
        "        sample_seed=42,\n",
        "    ),\n",
        "\n",
        "    text_splitter=List([\n",
        "            RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32),\n",
        "            RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=32),\n",
        "        ],\n",
        "    ),\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "    },\n",
        "    vector_store=None,  # Defaults to FAISS\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 20},\n",
        "\n",
        "    reranker_cls=CrossEncoderReranker,\n",
        "    reranker_kwargs={\n",
        "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cpu\"},\n",
        "        \"top_n\": List([5, 10]),\n",
        "    },\n",
        "    enable_gpu_search=True,\n",
        ")"
      ],
      "metadata": {
        "id": "m8jldNPpALMk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmtoJWa_4ZeA"
      },
      "source": [
        "Preprocess (retrieval-only focus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_preprocess_fn(batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager) -> Dict[str, listtype]:\n",
        "    INSTRUCTIONS = \"Utilize your knowledge of electronics to answer the following question based on the provided reviews.\"\n",
        "\n",
        "    batch_queries = [str(q).strip() for q in batch[\"query\"]]\n",
        "\n",
        "    # Perform retrieval\n",
        "    all_context = rag.get_context(batch_queries=batch_queries, serialize=False)\n",
        "\n",
        "    # Explicitly extract and cast IDs to strings to match QRELS\n",
        "    retrieved_documents = [\n",
        "        [str(doc.metadata.get(\"corpus_id\", \"\")).strip() for doc in docs]\n",
        "        for docs in all_context\n",
        "    ]\n",
        "\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "\n",
        "    return {\n",
        "        \"prompts\": [\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"},\n",
        "            ]\n",
        "            for question, context in zip(batch_queries, serialized_context)\n",
        "        ],\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "\n",
        "        **{k: list(v) for k, v in batch.items()},\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8x8s-W7BTDvF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m0b2Zfv4d5V"
      },
      "source": [
        "Postprocess (attach ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
        "    # Ensure qrels is strings for comparison\n",
        "    qrels['query_id'] = qrels['query_id'].astype(str).str.strip()\n",
        "    qrels['corpus_id'] = qrels['corpus_id'].astype(str).str.strip()\n",
        "\n",
        "    gt_docs = []\n",
        "    for qid in batch[\"query_id\"]:\n",
        "        target_qid = str(qid).strip()\n",
        "        relevant = qrels[qrels[\"query_id\"] == target_qid][\"corpus_id\"].tolist()\n",
        "        gt_docs.append(relevant)\n",
        "\n",
        "    batch[\"ground_truth_documents\"] = gt_docs\n",
        "    return batch"
      ],
      "metadata": {
        "id": "6FSooK7RTIFd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0dxiB64jVP"
      },
      "source": [
        "Metrics (Precision / Recall / MRR / NDCG)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ndcg_at_k(retrieved_docs, expected_docs, k=5):\n",
        "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
        "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
        "    ideal_length = min(k, len(expected_docs))\n",
        "    idcg = sum(1 / math.log2(i + 2) for i in range(ideal_length))\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    precisions, recalls, ndcgs, rrs, hits = [], [], [], [], []\n",
        "    total_queries = len(batch[\"query\"])\n",
        "\n",
        "    if total_queries > 0:\n",
        "        print(f\"DEBUG - Pred: {batch['retrieved_documents'][0]}\")\n",
        "        print(f\"DEBUG - GT: {batch['ground_truth_documents'][0]}\")\n",
        "\n",
        "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
        "        actual = set(str(p).strip() for p in pred)\n",
        "        expected = set(str(g).strip() for g in gt)\n",
        "\n",
        "        if not expected:\n",
        "            precisions.append(0); recalls.append(0); ndcgs.append(0); rrs.append(0); hits.append(0)\n",
        "            continue\n",
        "\n",
        "        tp = len(actual.intersection(expected))\n",
        "\n",
        "        precisions.append(tp / len(actual) if actual else 0)\n",
        "        recalls.append(tp / len(expected) if expected else 0)\n",
        "        ndcgs.append(compute_ndcg_at_k(pred, expected, k=5))\n",
        "\n",
        "        # Hit Rate: Did we get at least one review for the right product?\n",
        "        hits.append(1 if tp > 0 else 0)\n",
        "\n",
        "        # Reciprocal Rank calculation\n",
        "        rr = 0\n",
        "        for i, p in enumerate(pred):\n",
        "            if str(p).strip() in expected:\n",
        "                rr = 1 / (i + 1)\n",
        "                break\n",
        "        rrs.append(rr)\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"Hit Rate\": {\"value\": sum(hits) / total_queries}, # NEW\n",
        "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
        "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
        "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
        "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
        "    }\n",
        "\n",
        "def sample_accumulate_metrics_fn(aggregated_metrics: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    total_queries = sum(m[\"value\"] for m in aggregated_metrics[\"Total\"])\n",
        "    metrics = [\"Hit Rate\", \"Precision\", \"Recall\", \"NDCG@5\", \"MRR\"]\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        **{\n",
        "            m: {\n",
        "                \"value\": sum(v[\"value\"] for v in aggregated_metrics[m]) / len(aggregated_metrics[m]),\n",
        "                \"is_algebraic\": True\n",
        "            } for m in metrics\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "yUdTWpeBAnTm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yir3wRg44zYn"
      },
      "source": [
        "Grid + Experiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_config = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.25,\n",
        "        \"enforce_eager\": True,\n",
        "        \"max_model_len\": 2048,\n",
        "        \"disable_log_stats\": True,\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 128,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        ")\n",
        "\n",
        "config_set = {\n",
        "    \"vllm_config\": vllm_config,\n",
        "    \"batch_size\": 4,\n",
        "    \"preprocess_fn\": sample_preprocess_fn,\n",
        "    \"postprocess_fn\": sample_postprocess_fn,\n",
        "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
        "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
        "    \"online_strategy_kwargs\": {\n",
        "        \"strategy_name\": \"normal\",\n",
        "        \"confidence_level\": 0.95,\n",
        "        \"use_fpc\": True,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "zWzlFFquA5TN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_group = RFGridSearch(config_set)\n",
        "experiment = Experiment(experiment_name=\"amazon-electronics-rag-v2\", mode=\"evals\")\n",
        "\n",
        "results = experiment.run_evals(config_group=config_group, dataset=electronics_dataset, num_actors=1,num_shards=4,seed=42)\n",
        "\n",
        "experiment.end()\n"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8851/dispatcher/list-all-pipeline-ids": {
              "data": "W3sicGlwZWxpbmVfaWQiOjQsInNoYXJkc19jb21wbGV0ZWQiOjAsInN0YXR1cyI6Im5ldyIsInRvdGFsX3NhbXBsZXNfcHJvY2Vzc2VkIjowfSx7InBpcGVsaW5lX2lkIjozLCJzaGFyZHNfY29tcGxldGVkIjowLCJzdGF0dXMiOiJvbmdvaW5nIiwidG90YWxfc2FtcGxlc19wcm9jZXNzZWQiOjB9LHsicGlwZWxpbmVfaWQiOjIsInNoYXJkc19jb21wbGV0ZWQiOjEsInN0YXR1cyI6Im9uZ29pbmciLCJ0b3RhbF9zYW1wbGVzX3Byb2Nlc3NlZCI6MjV9LHsicGlwZWxpbmVfaWQiOjEsInNoYXJkc19jb21wbGV0ZWQiOjEsInN0YXR1cyI6Im9uZ29pbmciLCJ0b3RhbF9zYW1wbGVzX3Byb2Nlc3NlZCI6MjV9XQo=",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "344"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "status": 200,
              "status_text": ""
            },
            "http://localhost:8851/dispatcher/get-pipeline-config-json/4": {
              "data": "eyJjb250ZXh0X2lkIjoyLCJwaXBlbGluZV9jb25maWdfanNvbiI6eyJiYXRjaF9zaXplIjo0LCJtb2RlbF9jb25maWciOnsiZGlzYWJsZV9sb2dfc3RhdHMiOnRydWUsImR0eXBlIjoiaGFsZiIsImVuZm9yY2VfZWFnZXIiOnRydWUsImdwdV9tZW1vcnlfdXRpbGl6YXRpb24iOjAuMjUsIm1heF9tb2RlbF9sZW4iOjIwNDgsIm1vZGVsIjoiUXdlbi9Rd2VuMi41LTAuNUItSW5zdHJ1Y3QifSwib25saW5lX3N0cmF0ZWd5X2t3YXJncyI6eyJjb25maWRlbmNlX2xldmVsIjowLjk1LCJzdHJhdGVneV9uYW1lIjoibm9ybWFsIiwidXNlX2ZwYyI6dHJ1ZX0sInBpcGVsaW5lX3R5cGUiOiJ2bGxtIiwicmFnX2NvbmZpZyI6eyJjaHVua19vdmVybGFwIjozMiwiY2h1bmtfc2l6ZSI6MTI4LCJrIjoyMCwic2VhcmNoX3R5cGUiOiJzaW1pbGFyaXR5IiwidG9wX24iOjEwfSwic2FtcGxpbmdfcGFyYW1zIjp7Im1heF90b2tlbnMiOjEyOCwidGVtcGVyYXR1cmUiOjAuNywidG9wX3AiOjAuOTV9fX0K",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "501"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9TFlxSgGA_zA",
        "outputId": "b92bd815-a6c2-4972-f845-b9c998941c17"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/rapidfireai/logs/amazon-electronics-rag-v2_2\n",
            "The previously running experiment amazon-electronics-rag-v2_2 was forcibly ended. Created a new experiment 'amazon-electronics-rag-v2_3' with Experiment ID: 4 at /content/rapidfireai/rapidfire_experiments/amazon-electronics-rag-v2_3\n",
            "üåê Google Colab detected. Ray dashboard URL: https://8855-gpu-t4-hm-1lpb172kzqkzg-c.asia-southeast1-1.prod.colab.dev\n",
            "üåê Google Colab detected. Dispatcher URL: https://8851-gpu-t4-hm-1lpb172kzqkzg-c.asia-southeast1-1.prod.colab.dev\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div id=\"controller_b0e53bd6\" style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto;\">\n",
              "            <style>\n",
              "                #controller_b0e53bd6 h3 { margin: 10px 0; font-size: 1.2em; font-weight: 600; }\n",
              "                #controller_b0e53bd6 .header-info { display: flex; gap: 20px; margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px; font-size: 13px; }\n",
              "                #controller_b0e53bd6 .section { margin: 15px 0; }\n",
              "                #controller_b0e53bd6 .section-label { font-weight: 600; margin-bottom: 8px; font-size: 14px; }\n",
              "                #controller_b0e53bd6 .button-row { display: flex; gap: 8px; flex-wrap: wrap; margin: 10px 0; }\n",
              "                #controller_b0e53bd6 select { padding: 6px 12px; border: 1px solid #ccc; border-radius: 4px; font-size: 13px; background: white; min-width: 300px; cursor: pointer; }\n",
              "                #controller_b0e53bd6 button { padding: 6px 16px; border: none; border-radius: 4px; font-size: 13px; font-weight: 500; cursor: pointer; }\n",
              "                #controller_b0e53bd6 button:disabled { opacity: 0.5; cursor: not-allowed; }\n",
              "                #controller_b0e53bd6 .btn-success { background: #28a745; color: white; }\n",
              "                #controller_b0e53bd6 .btn-danger { background: #dc3545; color: white; }\n",
              "                #controller_b0e53bd6 .btn-info { background: #17a2b8; color: white; }\n",
              "                #controller_b0e53bd6 .btn-default { background: #6c757d; color: white; }\n",
              "                #controller_b0e53bd6 textarea { width: 100%; min-height: 200px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 12px; box-sizing: border-box; }\n",
              "                #controller_b0e53bd6 .status-message { padding: 10px; margin: 10px 0; border-radius: 4px; display: none; }\n",
              "                #controller_b0e53bd6 .msg-success { background: #d4edda; color: #155724; }\n",
              "                #controller_b0e53bd6 .msg-error { background: #f8d7da; color: #721c24; }\n",
              "                #controller_b0e53bd6 .msg-info { background: #d1ecf1; color: #0c5460; }\n",
              "            </style>\n",
              "\n",
              "            <div>\n",
              "                <h3>Interactive Run Controller</h3>\n",
              "                <div class=\"header-info\">\n",
              "                    <div><b>Run ID:</b> <span id=\"pipeline-id-value\">N/A</span></div>\n",
              "                    <div><b>Status:</b> <span id=\"status-value\">Not loaded</span></div>\n",
              "                    <div><b>Last Update:</b> <span id=\"last-update\">Never</span></div>\n",
              "                </div>\n",
              "\n",
              "                <div id=\"status-message\" class=\"status-message\"></div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Select a Config ID:</div>\n",
              "                    <select id=\"pipeline-selector\">\n",
              "                        <option value=\"\">Waiting for data...</option>\n",
              "                    </select>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-success\" id=\"resume-btn\">‚ñ∂ Resume</button>\n",
              "                        <button class=\"btn-danger\" id=\"stop-btn\">‚ñ† Stop</button>\n",
              "                        <button class=\"btn-danger\" id=\"delete-btn\">üóë Delete</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Configuration: <span id=\"config-name\">N/A</span></div>\n",
              "                    <textarea id=\"config-text\" readonly>{}</textarea>\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-info\" id=\"clone-btn\">Clone Run</button>\n",
              "                        <button class=\"btn-success\" id=\"submit-clone-btn\" disabled>‚úì Submit Clone</button>\n",
              "                        <button class=\"btn-danger\" id=\"cancel-clone-btn\" disabled>‚úó Cancel</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "            </div>\n",
              "\n",
              "            <script>\n",
              "                (function() {\n",
              "                    const WIDGET_ID = 'controller_b0e53bd6';\n",
              "                    const DISPATCHER_URL = 'https://localhost:8851';\n",
              "                    let currentPipelineId = null;\n",
              "                    let currentConfig = null;\n",
              "                    let currentContextId = null;\n",
              "                    let isCloneMode = false;\n",
              "                    let pollingInterval = null;\n",
              "\n",
              "                    // Elements\n",
              "                    const el = {\n",
              "                        pipelineIdValue: document.getElementById('pipeline-id-value'),\n",
              "                        statusValue: document.getElementById('status-value'),\n",
              "                        lastUpdate: document.getElementById('last-update'),\n",
              "                        statusMessage: document.getElementById('status-message'),\n",
              "                        pipelineSelector: document.getElementById('pipeline-selector'),\n",
              "                        resumeBtn: document.getElementById('resume-btn'),\n",
              "                        stopBtn: document.getElementById('stop-btn'),\n",
              "                        deleteBtn: document.getElementById('delete-btn'),\n",
              "                        configName: document.getElementById('config-name'),\n",
              "                        configText: document.getElementById('config-text'),\n",
              "                        cloneBtn: document.getElementById('clone-btn'),\n",
              "                        submitCloneBtn: document.getElementById('submit-clone-btn'),\n",
              "                        cancelCloneBtn: document.getElementById('cancel-clone-btn')\n",
              "                    };\n",
              "\n",
              "                    // Use fetch API with explicit CORS mode and optional auth token\n",
              "                    async function xhrRequest(url, method = 'GET', body = null) {\n",
              "                        const options = {\n",
              "                            method: method,\n",
              "                            headers: {\n",
              "                                'Content-Type': 'application/json'\n",
              "                            },\n",
              "                            mode: 'cors',\n",
              "                            credentials: 'include'  // Include cookies for Colab proxy auth\n",
              "                        };\n",
              "\n",
              "                        if (body) {\n",
              "                            options.body = JSON.stringify(body);\n",
              "                        }\n",
              "\n",
              "                        const response = await fetch(url, options);\n",
              "                        if (!response.ok) {\n",
              "                            throw new Error('HTTP ' + response.status);\n",
              "                        }\n",
              "                        return await response.json();\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelines() {\n",
              "                        try {\n",
              "                            console.log('Fetching pipelines...');\n",
              "                            const pipelines = await xhrRequest(DISPATCHER_URL + '/dispatcher/list-all-pipeline-ids');\n",
              "                            console.log('Got pipelines:', pipelines.length);\n",
              "\n",
              "                            updatePipelinesDropdown(pipelines);\n",
              "                            el.lastUpdate.textContent = new Date().toLocaleTimeString();\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch pipelines:', error);\n",
              "                            showMessage('Connection error: ' + error.message, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelineConfig(pipelineId) {\n",
              "                        try {\n",
              "                            const data = await xhrRequest(DISPATCHER_URL + `/dispatcher/get-pipeline-config-json/${pipelineId}`);\n",
              "                            const config = data.pipeline_config_json || {};\n",
              "\n",
              "                            currentConfig = config;\n",
              "                            currentContextId = data.context_id;\n",
              "\n",
              "                            el.configName.textContent = config.pipeline_name || 'N/A';\n",
              "\n",
              "                            if (!isCloneMode) {\n",
              "                                el.configText.value = JSON.stringify(config, null, 2);\n",
              "                            }\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch config:', error);\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function updatePipelinesDropdown(pipelines) {\n",
              "                        const selector = el.pipelineSelector;\n",
              "                        const currentSelection = selector.value;\n",
              "\n",
              "                        selector.innerHTML = '';\n",
              "\n",
              "                        if (pipelines && pipelines.length > 0) {\n",
              "                            pipelines.forEach(p => {\n",
              "                                const option = document.createElement('option');\n",
              "                                option.value = p.pipeline_id;\n",
              "                                option.textContent = `Config ID: ${p.pipeline_id} (${p.status || 'unknown'})`;\n",
              "                                selector.appendChild(option);\n",
              "                            });\n",
              "\n",
              "                            if (currentSelection && pipelines.some(p => p.pipeline_id == currentSelection)) {\n",
              "                                selector.value = currentSelection;\n",
              "                                currentPipelineId = currentSelection;\n",
              "                            } else {\n",
              "                                selector.value = pipelines[0].pipeline_id;\n",
              "                                currentPipelineId = pipelines[0].pipeline_id;\n",
              "                                fetchPipelineConfig(currentPipelineId);\n",
              "                            }\n",
              "\n",
              "                            // Update status display\n",
              "                            const currentPipeline = pipelines.find(p => p.pipeline_id == currentPipelineId);\n",
              "                            if (currentPipeline) {\n",
              "                                el.pipelineIdValue.textContent = currentPipeline.pipeline_id;\n",
              "                                el.statusValue.textContent = currentPipeline.status || 'unknown';\n",
              "\n",
              "                                const isCompleted = currentPipeline.status?.toLowerCase() === 'completed';\n",
              "                                el.resumeBtn.disabled = isCompleted;\n",
              "                                el.stopBtn.disabled = isCompleted;\n",
              "                                el.deleteBtn.disabled = isCompleted;\n",
              "                                el.cloneBtn.disabled = isCompleted || !currentContextId;\n",
              "                            }\n",
              "                        } else {\n",
              "                            selector.innerHTML = '<option value=\"\">No pipelines found</option>';\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function showMessage(message, type) {\n",
              "                        el.statusMessage.className = 'status-message msg-' + type;\n",
              "                        el.statusMessage.textContent = message;\n",
              "                        el.statusMessage.style.display = 'block';\n",
              "                        setTimeout(() => el.statusMessage.style.display = 'none', 5000);\n",
              "                    }\n",
              "\n",
              "                    async function handleAction(action) {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            const endpoint = DISPATCHER_URL + `/dispatcher/${action}-pipeline`;\n",
              "                            const result = await xhrRequest(endpoint, 'POST', { pipeline_id: currentPipelineId });\n",
              "\n",
              "                            showMessage(`‚úì ${action} completed for pipeline ${currentPipelineId}`, 'success');\n",
              "\n",
              "                            // Refresh after a short delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 500);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function enableCloneMode() {\n",
              "                        isCloneMode = true;\n",
              "                        el.configText.readOnly = false;\n",
              "                        el.submitCloneBtn.disabled = false;\n",
              "                        el.cancelCloneBtn.disabled = false;\n",
              "                        el.cloneBtn.disabled = true;\n",
              "                        showMessage('Edit config and click Submit to clone', 'info');\n",
              "                    }\n",
              "\n",
              "                    function disableCloneMode() {\n",
              "                        isCloneMode = false;\n",
              "                        el.configText.readOnly = true;\n",
              "                        el.configText.value = JSON.stringify(currentConfig || {}, null, 2);\n",
              "                        el.submitCloneBtn.disabled = true;\n",
              "                        el.cancelCloneBtn.disabled = true;\n",
              "                        el.cloneBtn.disabled = false;\n",
              "                    }\n",
              "\n",
              "                    async function handleClone() {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            // Parse edited config\n",
              "                            let editedConfig;\n",
              "                            try {\n",
              "                                editedConfig = JSON.parse(el.configText.value);\n",
              "                            } catch (e) {\n",
              "                                showMessage('Invalid JSON: ' + e.message, 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Validate required fields\n",
              "                            if (!editedConfig.pipeline_type) {\n",
              "                                showMessage('config_json must include pipeline_type', 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Send clone request\n",
              "                            const cloneRequest = {\n",
              "                                parent_pipeline_id: currentPipelineId,\n",
              "                                config_json: editedConfig\n",
              "                            };\n",
              "\n",
              "                            const result = await xhrRequest(\n",
              "                                DISPATCHER_URL + '/dispatcher/clone-pipeline',\n",
              "                                'POST',\n",
              "                                cloneRequest\n",
              "                            );\n",
              "\n",
              "                            showMessage(`‚úì Cloned from Config ID ${currentPipelineId} successfully!`, 'success');\n",
              "                            disableCloneMode();\n",
              "\n",
              "                            // Refresh after delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 1000);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error cloning: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    // Event listeners\n",
              "                    el.pipelineSelector.addEventListener('change', (e) => {\n",
              "                        if (e.target.value) {\n",
              "                            currentPipelineId = parseInt(e.target.value);\n",
              "                            fetchPipelineConfig(currentPipelineId);\n",
              "                        }\n",
              "                    });\n",
              "\n",
              "                    el.resumeBtn.addEventListener('click', () => handleAction('resume'));\n",
              "                    el.stopBtn.addEventListener('click', () => handleAction('stop'));\n",
              "                    el.deleteBtn.addEventListener('click', () => handleAction('delete'));\n",
              "\n",
              "                    el.cloneBtn.addEventListener('click', enableCloneMode);\n",
              "                    el.submitCloneBtn.addEventListener('click', handleClone);\n",
              "                    el.cancelCloneBtn.addEventListener('click', () => {\n",
              "                        disableCloneMode();\n",
              "                        showMessage('Cancelled clone', 'info');\n",
              "                    });\n",
              "\n",
              "                    // Initial fetch\n",
              "                    console.log('UI initialized, fetching initial data...');\n",
              "                    setTimeout(async () => {\n",
              "                        await fetchPipelines();\n",
              "\n",
              "                        // Start polling - use HTTP fetch (works even when kernel is busy)\n",
              "                        pollingInterval = setInterval(async () => {\n",
              "                            await fetchPipelines();\n",
              "                        }, 3000.0);\n",
              "                        console.log('Polling started: every 3.0s');\n",
              "                    }, 1000);\n",
              "\n",
              "                    // Cleanup on unload\n",
              "                    window.addEventListener('beforeunload', () => {\n",
              "                        if (pollingInterval) clearInterval(pollingInterval);\n",
              "                    });\n",
              "                })();\n",
              "            </script>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Preprocessing RAG Sources ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f842c2f2180>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_d68a8\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_d68a8_level0_col0\" class=\"col_heading level0 col0\" >RAG Source ID</th>\n",
              "      <th id=\"T_d68a8_level0_col1\" class=\"col_heading level0 col1\" >Status</th>\n",
              "      <th id=\"T_d68a8_level0_col2\" class=\"col_heading level0 col2\" >Duration</th>\n",
              "      <th id=\"T_d68a8_level0_col3\" class=\"col_heading level0 col3\" >Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_d68a8_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_d68a8_row0_col1\" class=\"data row0 col1\" >Complete</td>\n",
              "      <td id=\"T_d68a8_row0_col2\" class=\"data row0 col2\" >23.2s</td>\n",
              "      <td id=\"T_d68a8_row0_col3\" class=\"data row0 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d68a8_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_d68a8_row1_col1\" class=\"data row1 col1\" >Complete</td>\n",
              "      <td id=\"T_d68a8_row1_col2\" class=\"data row1 col2\" >23.8s</td>\n",
              "      <td id=\"T_d68a8_row1_col3\" class=\"data row1 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Multi-Config Experiment Progress ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f842c232a80>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_acd24\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_acd24_level0_col0\" class=\"col_heading level0 col0\" >Run ID</th>\n",
              "      <th id=\"T_acd24_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
              "      <th id=\"T_acd24_level0_col2\" class=\"col_heading level0 col2\" >Status</th>\n",
              "      <th id=\"T_acd24_level0_col3\" class=\"col_heading level0 col3\" >Progress</th>\n",
              "      <th id=\"T_acd24_level0_col4\" class=\"col_heading level0 col4\" >Conf. Interval</th>\n",
              "      <th id=\"T_acd24_level0_col5\" class=\"col_heading level0 col5\" >search_type</th>\n",
              "      <th id=\"T_acd24_level0_col6\" class=\"col_heading level0 col6\" >rag_k</th>\n",
              "      <th id=\"T_acd24_level0_col7\" class=\"col_heading level0 col7\" >top_n</th>\n",
              "      <th id=\"T_acd24_level0_col8\" class=\"col_heading level0 col8\" >chunk_size</th>\n",
              "      <th id=\"T_acd24_level0_col9\" class=\"col_heading level0 col9\" >chunk_overlap</th>\n",
              "      <th id=\"T_acd24_level0_col10\" class=\"col_heading level0 col10\" >sampling_params</th>\n",
              "      <th id=\"T_acd24_level0_col11\" class=\"col_heading level0 col11\" >model_config</th>\n",
              "      <th id=\"T_acd24_level0_col12\" class=\"col_heading level0 col12\" >Precision</th>\n",
              "      <th id=\"T_acd24_level0_col13\" class=\"col_heading level0 col13\" >Recall</th>\n",
              "      <th id=\"T_acd24_level0_col14\" class=\"col_heading level0 col14\" >NDCG@5</th>\n",
              "      <th id=\"T_acd24_level0_col15\" class=\"col_heading level0 col15\" >MRR</th>\n",
              "      <th id=\"T_acd24_level0_col16\" class=\"col_heading level0 col16\" >Throughput</th>\n",
              "      <th id=\"T_acd24_level0_col17\" class=\"col_heading level0 col17\" >Total</th>\n",
              "      <th id=\"T_acd24_level0_col18\" class=\"col_heading level0 col18\" >Samples Processed</th>\n",
              "      <th id=\"T_acd24_level0_col19\" class=\"col_heading level0 col19\" >Hit Rate</th>\n",
              "      <th id=\"T_acd24_level0_col20\" class=\"col_heading level0 col20\" >Processing Time</th>\n",
              "      <th id=\"T_acd24_level0_col21\" class=\"col_heading level0 col21\" >Samples Per Second</th>\n",
              "      <th id=\"T_acd24_level0_col22\" class=\"col_heading level0 col22\" >model_name</th>\n",
              "      <th id=\"T_acd24_level0_col23\" class=\"col_heading level0 col23\" >run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_acd24_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_acd24_row0_col1\" class=\"data row0 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row0_col2\" class=\"data row0 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_acd24_row0_col3\" class=\"data row0 col3\" >4/4</td>\n",
              "      <td id=\"T_acd24_row0_col4\" class=\"data row0 col4\" >0.000</td>\n",
              "      <td id=\"T_acd24_row0_col5\" class=\"data row0 col5\" >similarity</td>\n",
              "      <td id=\"T_acd24_row0_col6\" class=\"data row0 col6\" >20.00</td>\n",
              "      <td id=\"T_acd24_row0_col7\" class=\"data row0 col7\" >5.00</td>\n",
              "      <td id=\"T_acd24_row0_col8\" class=\"data row0 col8\" >256.00</td>\n",
              "      <td id=\"T_acd24_row0_col9\" class=\"data row0 col9\" >32.00</td>\n",
              "      <td id=\"T_acd24_row0_col10\" class=\"data row0 col10\" >{'temperature': 0.7, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_acd24_row0_col11\" class=\"data row0 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'enforce_eager': True, 'max_model_len': 2048, 'disable_log_stats': True}</td>\n",
              "      <td id=\"T_acd24_row0_col12\" class=\"data row0 col12\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row0_col13\" class=\"data row0 col13\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row0_col14\" class=\"data row0 col14\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row0_col15\" class=\"data row0 col15\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row0_col16\" class=\"data row0 col16\" >0.3/s</td>\n",
              "      <td id=\"T_acd24_row0_col17\" class=\"data row0 col17\" >100</td>\n",
              "      <td id=\"T_acd24_row0_col18\" class=\"data row0 col18\" >100</td>\n",
              "      <td id=\"T_acd24_row0_col19\" class=\"data row0 col19\" >0.0000 [0.0000, 0.0000]</td>\n",
              "      <td id=\"T_acd24_row0_col20\" class=\"data row0 col20\" >438.55 seconds</td>\n",
              "      <td id=\"T_acd24_row0_col21\" class=\"data row0 col21\" >0.23</td>\n",
              "      <td id=\"T_acd24_row0_col22\" class=\"data row0 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row0_col23\" class=\"data row0 col23\" >1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_acd24_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_acd24_row1_col1\" class=\"data row1 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row1_col2\" class=\"data row1 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_acd24_row1_col3\" class=\"data row1 col3\" >4/4</td>\n",
              "      <td id=\"T_acd24_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
              "      <td id=\"T_acd24_row1_col5\" class=\"data row1 col5\" >similarity</td>\n",
              "      <td id=\"T_acd24_row1_col6\" class=\"data row1 col6\" >20.00</td>\n",
              "      <td id=\"T_acd24_row1_col7\" class=\"data row1 col7\" >10.00</td>\n",
              "      <td id=\"T_acd24_row1_col8\" class=\"data row1 col8\" >256.00</td>\n",
              "      <td id=\"T_acd24_row1_col9\" class=\"data row1 col9\" >32.00</td>\n",
              "      <td id=\"T_acd24_row1_col10\" class=\"data row1 col10\" >{'temperature': 0.7, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_acd24_row1_col11\" class=\"data row1 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'enforce_eager': True, 'max_model_len': 2048, 'disable_log_stats': True}</td>\n",
              "      <td id=\"T_acd24_row1_col12\" class=\"data row1 col12\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row1_col13\" class=\"data row1 col13\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row1_col14\" class=\"data row1 col14\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row1_col15\" class=\"data row1 col15\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row1_col16\" class=\"data row1 col16\" >0.3/s</td>\n",
              "      <td id=\"T_acd24_row1_col17\" class=\"data row1 col17\" >100</td>\n",
              "      <td id=\"T_acd24_row1_col18\" class=\"data row1 col18\" >100</td>\n",
              "      <td id=\"T_acd24_row1_col19\" class=\"data row1 col19\" >0.0000 [0.0000, 0.0000]</td>\n",
              "      <td id=\"T_acd24_row1_col20\" class=\"data row1 col20\" >351.66 seconds</td>\n",
              "      <td id=\"T_acd24_row1_col21\" class=\"data row1 col21\" >0.28</td>\n",
              "      <td id=\"T_acd24_row1_col22\" class=\"data row1 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row1_col23\" class=\"data row1 col23\" >2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_acd24_row2_col0\" class=\"data row2 col0\" >3</td>\n",
              "      <td id=\"T_acd24_row2_col1\" class=\"data row2 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row2_col2\" class=\"data row2 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_acd24_row2_col3\" class=\"data row2 col3\" >4/4</td>\n",
              "      <td id=\"T_acd24_row2_col4\" class=\"data row2 col4\" >0.000</td>\n",
              "      <td id=\"T_acd24_row2_col5\" class=\"data row2 col5\" >similarity</td>\n",
              "      <td id=\"T_acd24_row2_col6\" class=\"data row2 col6\" >20.00</td>\n",
              "      <td id=\"T_acd24_row2_col7\" class=\"data row2 col7\" >5.00</td>\n",
              "      <td id=\"T_acd24_row2_col8\" class=\"data row2 col8\" >128.00</td>\n",
              "      <td id=\"T_acd24_row2_col9\" class=\"data row2 col9\" >32.00</td>\n",
              "      <td id=\"T_acd24_row2_col10\" class=\"data row2 col10\" >{'temperature': 0.7, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_acd24_row2_col11\" class=\"data row2 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'enforce_eager': True, 'max_model_len': 2048, 'disable_log_stats': True}</td>\n",
              "      <td id=\"T_acd24_row2_col12\" class=\"data row2 col12\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row2_col13\" class=\"data row2 col13\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row2_col14\" class=\"data row2 col14\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row2_col15\" class=\"data row2 col15\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row2_col16\" class=\"data row2 col16\" >0.3/s</td>\n",
              "      <td id=\"T_acd24_row2_col17\" class=\"data row2 col17\" >100</td>\n",
              "      <td id=\"T_acd24_row2_col18\" class=\"data row2 col18\" >100</td>\n",
              "      <td id=\"T_acd24_row2_col19\" class=\"data row2 col19\" >0.0000 [0.0000, 0.0000]</td>\n",
              "      <td id=\"T_acd24_row2_col20\" class=\"data row2 col20\" >325.98 seconds</td>\n",
              "      <td id=\"T_acd24_row2_col21\" class=\"data row2 col21\" >0.31</td>\n",
              "      <td id=\"T_acd24_row2_col22\" class=\"data row2 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row2_col23\" class=\"data row2 col23\" >3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_acd24_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_acd24_row3_col1\" class=\"data row3 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row3_col2\" class=\"data row3 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_acd24_row3_col3\" class=\"data row3 col3\" >4/4</td>\n",
              "      <td id=\"T_acd24_row3_col4\" class=\"data row3 col4\" >0.000</td>\n",
              "      <td id=\"T_acd24_row3_col5\" class=\"data row3 col5\" >similarity</td>\n",
              "      <td id=\"T_acd24_row3_col6\" class=\"data row3 col6\" >20.00</td>\n",
              "      <td id=\"T_acd24_row3_col7\" class=\"data row3 col7\" >10.00</td>\n",
              "      <td id=\"T_acd24_row3_col8\" class=\"data row3 col8\" >128.00</td>\n",
              "      <td id=\"T_acd24_row3_col9\" class=\"data row3 col9\" >32.00</td>\n",
              "      <td id=\"T_acd24_row3_col10\" class=\"data row3 col10\" >{'temperature': 0.7, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_acd24_row3_col11\" class=\"data row3 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'enforce_eager': True, 'max_model_len': 2048, 'disable_log_stats': True}</td>\n",
              "      <td id=\"T_acd24_row3_col12\" class=\"data row3 col12\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row3_col13\" class=\"data row3 col13\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row3_col14\" class=\"data row3 col14\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row3_col15\" class=\"data row3 col15\" >0.00% [0.00%, 0.00%]</td>\n",
              "      <td id=\"T_acd24_row3_col16\" class=\"data row3 col16\" >0.3/s</td>\n",
              "      <td id=\"T_acd24_row3_col17\" class=\"data row3 col17\" >100</td>\n",
              "      <td id=\"T_acd24_row3_col18\" class=\"data row3 col18\" >100</td>\n",
              "      <td id=\"T_acd24_row3_col19\" class=\"data row3 col19\" >0.0000 [0.0000, 0.0000]</td>\n",
              "      <td id=\"T_acd24_row3_col20\" class=\"data row3 col20\" >305.03 seconds</td>\n",
              "      <td id=\"T_acd24_row3_col21\" class=\"data row3 col21\" >0.33</td>\n",
              "      <td id=\"T_acd24_row3_col22\" class=\"data row3 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_acd24_row3_col23\" class=\"data row3 col23\" >4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment amazon-electronics-rag-v2_3 ended\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzmJKUwR4_H8"
      },
      "source": [
        "Results table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([{k: (v['value'] if isinstance(v, dict) else v) for k, v in {**m, 'run_id': rid}.items()} for rid, (_, m) in results.items()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "80zku7BACCf9",
        "outputId": "06e2c5e2-21b3-418a-e7a0-a156ff5a4574"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   run_id                  model_name search_type  rag_k  top_n  chunk_size  \\\n",
              "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity     20      5         256   \n",
              "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity     20     10         256   \n",
              "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity     20      5         128   \n",
              "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity     20     10         128   \n",
              "\n",
              "   chunk_overlap                                    sampling_params  \\\n",
              "0             32  {'temperature': 0.7, 'top_p': 0.95, 'max_token...   \n",
              "1             32  {'temperature': 0.7, 'top_p': 0.95, 'max_token...   \n",
              "2             32  {'temperature': 0.7, 'top_p': 0.95, 'max_token...   \n",
              "3             32  {'temperature': 0.7, 'top_p': 0.95, 'max_token...   \n",
              "\n",
              "                                        model_config  Samples Processed  \\\n",
              "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                100   \n",
              "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                100   \n",
              "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                100   \n",
              "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                100   \n",
              "\n",
              "  Processing Time Samples Per Second  Total  Hit Rate  Precision  Recall  \\\n",
              "0  438.55 seconds               0.23    100       0.0        0.0     0.0   \n",
              "1  351.66 seconds               0.28    100       0.0        0.0     0.0   \n",
              "2  325.98 seconds               0.31    100       0.0        0.0     0.0   \n",
              "3  305.03 seconds               0.33    100       0.0        0.0     0.0   \n",
              "\n",
              "   NDCG@5  MRR  \n",
              "0     0.0  0.0  \n",
              "1     0.0  0.0  \n",
              "2     0.0  0.0  \n",
              "3     0.0  0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a4cc4a3-03b4-467b-bc6a-4cdd097e4a63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>search_type</th>\n",
              "      <th>rag_k</th>\n",
              "      <th>top_n</th>\n",
              "      <th>chunk_size</th>\n",
              "      <th>chunk_overlap</th>\n",
              "      <th>sampling_params</th>\n",
              "      <th>model_config</th>\n",
              "      <th>Samples Processed</th>\n",
              "      <th>Processing Time</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Total</th>\n",
              "      <th>Hit Rate</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>NDCG@5</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>256</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.7, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>100</td>\n",
              "      <td>438.55 seconds</td>\n",
              "      <td>0.23</td>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>256</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.7, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>100</td>\n",
              "      <td>351.66 seconds</td>\n",
              "      <td>0.28</td>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.7, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>100</td>\n",
              "      <td>325.98 seconds</td>\n",
              "      <td>0.31</td>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>128</td>\n",
              "      <td>32</td>\n",
              "      <td>{'temperature': 0.7, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>100</td>\n",
              "      <td>305.03 seconds</td>\n",
              "      <td>0.33</td>\n",
              "      <td>100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a4cc4a3-03b4-467b-bc6a-4cdd097e4a63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a4cc4a3-03b4-467b-bc6a-4cdd097e4a63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a4cc4a3-03b4-467b-bc6a-4cdd097e4a63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Qwen/Qwen2.5-0.5B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"similarity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 20,\n        \"max\": 20,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73,\n        \"min\": 128,\n        \"max\": 256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_overlap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 32,\n        \"max\": 32,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sampling_params\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_config\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Processed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processing Time\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"351.66 seconds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Per Second\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"0.28\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 100,\n        \"max\": 100,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hit Rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NDCG@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MRR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}