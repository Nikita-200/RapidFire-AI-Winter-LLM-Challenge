{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jc8dMxjZ3xCO"
   },
   "source": [
    "Install & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0oY3gK10vUX",
    "outputId": "f6c12cbc-275a-4d56-9f69-d99a62e3d83d"
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDdqQeCGyd8i",
    "outputId": "d9bcca04-4859-4752-fcd9-8fbb9e3db4ae"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai\n",
    "    print(\" rapidfireai installed\")\n",
    "except ImportError:\n",
    "    !pip install rapidfireai datasets==3.6.0 langchain sentence-transformers PyPDF2\n",
    "    !rapidfireai init --evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6TrfRa31-E"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNx1X8poyjyj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List as listtype, Dict, Any\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQFcu2iyUAi"
   },
   "source": [
    "DOCUGAMI FINANCIAL QA BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxqy_fYRyxBK"
   },
   "source": [
    "Setup and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93YMUZk5yy9S",
    "outputId": "cfaac354-ece0-43e5-fd58-c1d030c5006a"
   },
   "outputs": [],
   "source": [
    "# Directories\n",
    "dataset_dir = Path(\"./financial_rag_benchmark\")\n",
    "dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "pdf_dir = Path(\"./pdfs\")  # Directory containing your PDFs like \"2022 Q3 AAPL.pdf\"\n",
    "# pdf_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\" Loading Docugami Financial QA Benchmark...\")\n",
    "\n",
    "# Load your CSV file with the QA data\n",
    "csv_file_path = \"qna_data.csv\"  # UPDATE THIS PATH\n",
    "\n",
    "# If you need to upload:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# csv_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(f\" Loaded {len(df)} questions from benchmark\")\n",
    "print(f\"\\nDataset columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n Sample question:\")\n",
    "print(df.head(1).to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy-eFBtty-Z3"
   },
   "source": [
    "Process PDF Documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5W5bWsNNy73t",
    "outputId": "7b69c4f3-8731-495b-b736-6f825ebc67d1"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüî® Processing PDF documents into chunks...\")\n",
    "\n",
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                page_text = page.extract_text()\n",
    "                text += f\"\\n[Page {page_num + 1}]\\n{page_text}\"\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"    Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_source_docs(source_docs_str):\n",
    "    \"\"\"\n",
    "    Parse source docs string like '*2023 Q3 AAPL*' or '*MSFT*'\n",
    "    Returns list of document identifiers\n",
    "    \"\"\"\n",
    "    if pd.isna(source_docs_str) or not source_docs_str:\n",
    "        return []\n",
    "\n",
    "    # Remove asterisks and split by comma\n",
    "    cleaned = source_docs_str.replace('*', '').strip()\n",
    "\n",
    "    # Split by comma if multiple sources\n",
    "    sources = [s.strip() for s in cleaned.split(',')]\n",
    "\n",
    "    return sources\n",
    "\n",
    "def match_pdf_files(source_pattern, pdf_files):\n",
    "    \"\"\"\n",
    "    Match PDF files based on source pattern\n",
    "    e.g., '*MSFT*' matches all MSFT files\n",
    "    e.g., '*2023 Q3 AAPL*' matches that specific file\n",
    "    \"\"\"\n",
    "    matched_files = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_name = pdf_file.stem  # Filename without extension\n",
    "\n",
    "        for pattern in source_pattern:\n",
    "            # Check if pattern matches\n",
    "            if pattern in pdf_name:\n",
    "                matched_files.append(pdf_file)\n",
    "                break\n",
    "\n",
    "    return matched_files\n",
    "\n",
    "# Get all PDF files\n",
    "if pdf_dir.exists():\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    print(f\"   Found {len(pdf_files)} PDF files\")\n",
    "else:\n",
    "    print(f\"    PDF directory not found: {pdf_dir}\")\n",
    "    print(f\"   Please create the directory and add your PDF files\")\n",
    "    pdf_files = []\n",
    "\n",
    "# Create corpus from PDFs\n",
    "corpus_dict = {}  # doc_id -> {text, metadata}\n",
    "doc_to_source_mapping = {}  # Maps doc_id to source PDF name\n",
    "doc_counter = 0\n",
    "\n",
    "# Process each PDF\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"   Processing: {pdf_file.name}\")\n",
    "\n",
    "    # Extract text\n",
    "    full_text = extract_pdf_text(pdf_file)\n",
    "\n",
    "    if not full_text or len(full_text) < 100:\n",
    "        print(f\"    Skipping {pdf_file.name} - insufficient text\")\n",
    "        continue\n",
    "\n",
    "    # Split into chunks using RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Larger chunks for financial documents\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "    # Create documents for each chunk\n",
    "    source_name = pdf_file.stem  # e.g., \"2023 Q3 AAPL\"\n",
    "\n",
    "    for chunk_idx, chunk_text in enumerate(chunks):\n",
    "        if len(chunk_text.strip()) < 50:  # Skip very small chunks\n",
    "            continue\n",
    "\n",
    "        doc_id = f\"doc_{doc_counter}\"\n",
    "        doc_counter += 1\n",
    "\n",
    "        # Store chunk with metadata\n",
    "        corpus_dict[doc_id] = {\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"source_file\": source_name,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        }\n",
    "\n",
    "        # Map doc_id to source\n",
    "        doc_to_source_mapping[doc_id] = source_name\n",
    "\n",
    "    print(f\"   ‚úì Created {len(chunks)} chunks from {pdf_file.name}\")\n",
    "\n",
    "print(f\"\\n Total corpus documents (chunks): {len(corpus_dict)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgD6af8jzLpV"
   },
   "source": [
    "Create Queries and Qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NptiDem5zOnu",
    "outputId": "5a51b626-77ba-404f-ce6b-ec30191722fc"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüî® Creating queries and relevance judgments (QRELS)...\")\n",
    "\n",
    "queries_list = []\n",
    "qrels_rows = []\n",
    "\n",
    "# Track question types\n",
    "question_type_counts = defaultdict(int)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if i % 50 == 0:\n",
    "        print(f\"   Processing question {i}/{len(df)}...\")\n",
    "\n",
    "    query_id = f\"q_{i}\"\n",
    "    question = str(row['Question']).strip()\n",
    "    source_docs_str = str(row['Source Docs']).strip()\n",
    "    question_type = str(row.get('Question Type', 'Unknown')).strip()\n",
    "\n",
    "    # Parse source documents\n",
    "    source_patterns = parse_source_docs(source_docs_str)\n",
    "\n",
    "    if not source_patterns:\n",
    "        print(f\"    Skipping question {i} - no source docs\")\n",
    "        continue\n",
    "\n",
    "    # Add query\n",
    "    queries_list.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"query\": question,\n",
    "        \"question_type\": question_type\n",
    "    })\n",
    "\n",
    "    # Track question type\n",
    "    question_type_counts[question_type] += 1\n",
    "\n",
    "    # Find all relevant document chunks (relevance=1)\n",
    "    relevant_doc_ids = []\n",
    "\n",
    "    for doc_id, doc_info in corpus_dict.items():\n",
    "        source_file = doc_info[\"source_file\"]\n",
    "\n",
    "        # Check if this chunk's source file matches any of the source patterns\n",
    "        is_relevant = False\n",
    "        for pattern in source_patterns:\n",
    "            if pattern in source_file:\n",
    "                is_relevant = True\n",
    "                break\n",
    "\n",
    "        if is_relevant:\n",
    "            relevant_doc_ids.append(doc_id)\n",
    "\n",
    "    # Add relevant documents to QRELS\n",
    "    for doc_id in relevant_doc_ids:\n",
    "        qrels_rows.append({\n",
    "            \"query_id\": query_id,\n",
    "            \"corpus_id\": doc_id,\n",
    "            \"relevance\": 1\n",
    "        })\n",
    "\n",
    "    # Add some irrelevant documents (relevance=0) for realistic evaluation\n",
    "    # Sample from documents NOT in the relevant set\n",
    "    irrelevant_doc_ids = [\n",
    "        doc_id for doc_id in corpus_dict.keys()\n",
    "        if doc_id not in relevant_doc_ids\n",
    "    ]\n",
    "\n",
    "    # Sample up to 10 irrelevant documents\n",
    "    import random\n",
    "    random.seed(42 + i)\n",
    "    num_negatives = min(10, len(irrelevant_doc_ids))\n",
    "\n",
    "    if num_negatives > 0:\n",
    "        sampled_negatives = random.sample(irrelevant_doc_ids, num_negatives)\n",
    "\n",
    "        for neg_doc_id in sampled_negatives:\n",
    "            qrels_rows.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"corpus_id\": neg_doc_id,\n",
    "                \"relevance\": 0\n",
    "            })\n",
    "\n",
    "print(f\"\\n Queries created: {len(queries_list)}\")\n",
    "print(f\"\\n Question Type Distribution:\")\n",
    "for q_type, count in sorted(question_type_counts.items()):\n",
    "    print(f\"   {q_type}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRTZKptlzSdZ"
   },
   "source": [
    "Save Data and Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "393UxybnzU3I",
    "outputId": "8190218a-7faf-4bc4-c926-07d69a3c2f2f"
   },
   "outputs": [],
   "source": [
    "print(\"\\n Saving corpus and creating dataframes...\")\n",
    "\n",
    "# Save corpus\n",
    "corpus_list = [\n",
    "    {\n",
    "        \"_id\": doc_id,\n",
    "        \"text\": info[\"text\"],\n",
    "        \"source_file\": info[\"source_file\"],\n",
    "        \"chunk_index\": info[\"chunk_index\"]\n",
    "    }\n",
    "    for doc_id, info in corpus_dict.items()\n",
    "]\n",
    "\n",
    "corpus_file = dataset_dir / \"corpus_sampled.jsonl\"\n",
    "with open(corpus_file, \"w\") as f:\n",
    "    for doc in corpus_list:\n",
    "        f.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "print(f\" Saved corpus to {corpus_file}\")\n",
    "\n",
    "# Create dataframes\n",
    "queries_df = pd.DataFrame(queries_list)\n",
    "queries_df['query_id'] = queries_df['query_id'].astype(str).str.strip()\n",
    "queries_df['query'] = queries_df['query'].astype(str).str.strip()\n",
    "\n",
    "qrels_df = pd.DataFrame(qrels_rows)\n",
    "qrels_df['query_id'] = qrels_df['query_id'].astype(str).str.strip()\n",
    "qrels_df['corpus_id'] = qrels_df['corpus_id'].astype(str).str.strip()\n",
    "qrels_df['relevance'] = qrels_df['relevance'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7HSojyTzdJm"
   },
   "source": [
    "Dataset Statistics and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NcB2XqMqPwom",
    "outputId": "94158ab0-82b2-457c-d850-498de4978291"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DOCUGAMI FINANCIAL QA BENCHMARK SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\" Total Queries: {len(queries_df)}\")\n",
    "print(f\" Total Corpus Documents (chunks): {len(corpus_list)}\")\n",
    "print(f\" Total QRELS entries: {len(qrels_df)}\")\n",
    "print(f\" Relevant documents (relevance=1): {len(qrels_df[qrels_df['relevance'] == 1])}\")\n",
    "print(f\" Irrelevant documents (relevance=0): {len(qrels_df[qrels_df['relevance'] == 0])}\")\n",
    "\n",
    "# Calculate statistics\n",
    "docs_per_query = qrels_df.groupby('query_id').size()\n",
    "print(f\"\\n Average documents per query: {docs_per_query.mean():.1f}\")\n",
    "\n",
    "relevant_per_query = qrels_df[qrels_df['relevance'] == 1].groupby('query_id').size()\n",
    "print(f\" Average relevant documents per query: {relevant_per_query.mean():.1f}\")\n",
    "print(f\" Min relevant docs: {relevant_per_query.min()}, Max: {relevant_per_query.max()}\")\n",
    "\n",
    "irrelevant_per_query = qrels_df[qrels_df['relevance'] == 0].groupby('query_id').size()\n",
    "print(f\" Average irrelevant documents per query: {irrelevant_per_query.mean():.1f}\")\n",
    "\n",
    "# Show sample for each question type\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" SAMPLE QUESTIONS BY TYPE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for q_type in question_type_counts.keys():\n",
    "    sample_query = queries_df[queries_df['question_type'] == q_type].iloc[0]\n",
    "    sample_qid = sample_query['query_id']\n",
    "\n",
    "    print(f\"\\n{q_type}:\")\n",
    "    print(f\"   Question: {sample_query['query'][:150]}...\")\n",
    "\n",
    "    sample_qrels = qrels_df[qrels_df['query_id'] == sample_qid]\n",
    "    relevant_docs = sample_qrels[sample_qrels['relevance'] == 1]['corpus_id'].tolist()\n",
    "\n",
    "    print(f\"   Relevant chunks: {len(relevant_docs)}\")\n",
    "\n",
    "    # Show which source files these chunks come from\n",
    "    source_files = set()\n",
    "    for doc_id in relevant_docs[:5]:  # Show first 5\n",
    "        source_file = corpus_dict[doc_id][\"source_file\"]\n",
    "        source_files.add(source_file)\n",
    "\n",
    "    print(f\"   Source files: {', '.join(list(source_files)[:3])}...\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDvR6QKY4Otx"
   },
   "source": [
    "Define RAG search space (retrieval-focused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErZ4cwM12a4P"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 50\n",
    "\n",
    "rag_config = RFLangChainRagSpec(\n",
    "    document_loader=DirectoryLoader(\n",
    "        path=str(dataset_dir),\n",
    "        glob=\"corpus_sampled.jsonl\",\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"metadata_func\": lambda record, metadata: {\n",
    "                \"corpus_id\": str(record.get(\"_id\", \"\")).strip(),\n",
    "                \"source_file\": str(record.get(\"source_file\", \"\")).strip(),\n",
    "                \"chunk_index\": record.get(\"chunk_index\", 0)\n",
    "            },\n",
    "            \"json_lines\": True,\n",
    "            \"text_content\": False,\n",
    "        },\n",
    "        sample_seed=42,\n",
    "    ),\n",
    "    # Use moderate chunking since PDFs are already pre-chunked\n",
    "    text_splitter= RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128\n",
    "    ),\n",
    "    # List([\n",
    "    #         RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    #             encoding_name=\"gpt2\", chunk_size=512, chunk_overlap=128\n",
    "    #         ),\n",
    "    #         RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    #             encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
    "    #         ),\n",
    "    #     ],\n",
    "    # ),\n",
    "\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"}, #\"cpu\"\n",
    "        \"encode_kwargs\": {\n",
    "            \"normalize_embeddings\": True,\n",
    "            \"batch_size\": batch_size\n",
    "        },\n",
    "    },\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 8},  # Retrieve more for multi-doc questions\n",
    "    reranker_cls=CrossEncoderReranker,\n",
    "    reranker_kwargs={\n",
    "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"}, #\"cpu\"\n",
    "        \"top_n\": 5,\n",
    "        # \"top_n\": List([3, 5]),  # Keep top 8 after reranking\n",
    "    },\n",
    "    enable_gpu_search=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbQZyWPjzxVA"
   },
   "source": [
    "### ‚ö†Ô∏è Important Note\n",
    "\n",
    "Running the code cell below **may trigger a pickling error** when `experiment.run_evals()` is executed.  \n",
    "This happens due to the invocation of `rag_config.build_index()`.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Usage\n",
    "\n",
    "##### To test document retrieval only\n",
    "- Uncomment the code cell below.\n",
    "- Run it to verify that the correct documents are being retrieved for your queries.\n",
    "- After testing, **restart the session and runtime**.\n",
    "\n",
    "##### To run RapidFireAI experiments\n",
    "- Keep the code cell below **commented out**.\n",
    "- Run the rest of the notebook as-is.\n",
    "- This avoids pickling issues during `experiment.run_evals()`.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Workflow\n",
    "1. *(Optional)* Uncomment and run the cell to test RAG document retrieval.\n",
    "2. Restart the session and runtime.\n",
    "3. Comment the cell again.\n",
    "4. Run the full notebook to execute RapidFireAI experiments safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0fb7114ca796484097360618bd88c7c3",
      "8a77fe2df24140f0ac5b50eb84eb362d",
      "ea14145e368744bc98232485b272a8eb",
      "727e9d35d2b74fa2bcc833ce037d01eb",
      "b54f0de072c84211ab8c9043608df726",
      "64ce2cfda1394b62b86354841deb22bb",
      "455775e2ff45477d84a5992b45ee6cda",
      "37023f4564824b6697c1171c3bb0866e",
      "87abb8dfdb9441d88f002a3286aeb41a",
      "374926fd38d64e10ab780aa4aab7c2c2",
      "f2e6e6c4b90a4ec48c1c336d52e7d351",
      "ff0d22d407c84276876e638198f5afaa",
      "7645d3171d5446c7b5173217d9ad9a5e",
      "936dcce4878c4576a6ff288fa8dd27f4",
      "6b1aae44ed5e45a48ef84af951ce8cb4",
      "d47ff6e6173a400d9e79eeff904388f7",
      "693581e68f3f42f4be1eacf65f869d1d",
      "7810d9fdfe6d45829a753deaea1857e4",
      "02709378753b42c0bc7d2515a3ad61f2",
      "b109040d99fc404695352193ad92b513",
      "06b777fcdfe046f89fd0fbb715d5f2e2",
      "d912f352d8fe434ea185ddbdd83cde9c",
      "21b2496506724dea870c157c355e8232",
      "cf450644a8104e63931b84a4a073fc48",
      "75145de92e8442f7801dc26257b4425b",
      "32fc91fd5d4446d9a630d170fdbb21a4",
      "7bd9ebab6df04742bd1f42fa9f65cc63",
      "048dae97680a433a9ab012faf259bd07",
      "c50cbc8bcdb947f091b500183a0c26e6",
      "4b0a0e333f6d48b784de3308beec44d9",
      "3d3b31f8c55f4938b1d071f9d8c58d3f",
      "62f22c8e11d84c9d91d5f3297c1ae7ad",
      "fa5080410d0f4092abd0417f4f169226",
      "83846324347f41c38570071a3f7a4fb9",
      "ba7facd778834f11abeb6fdf97a69ce0",
      "bd6be173d9604cef8be251751814f783",
      "bb81c78a61c64976953c6770e0e188a7",
      "b86da6af087940a7b25de687a86d8fb3",
      "cea6338b4450491daea82e3079542dc0",
      "77f730e4e2764d63921b9f308b0c7dba",
      "ad1c4094c3b04acda4e5c0ba52653015",
      "07a2ed87edc548e3a9ddf82c27820fb8",
      "e36f07c19a8d43008051b1795e10ef0b",
      "a6a8e29d64f4475c959335133d1948e1",
      "defddf3903dc479c9ee578f46949648f",
      "452bc342db704aa784102804681c306b",
      "6012cfe93cb1498d8f6ca12c585754fd",
      "55006432eb6c48a8b9530f7066f74f44",
      "237daae417a5499d98f87d78c063da24",
      "b793364ac46343ecb9b919b27b2560c9",
      "b819aee5d0224dd7b5d25681e7e6b6bf",
      "d05aed59c75444909afda0473e5a2b30",
      "b1eef479ddd643e5a9c6540d5c85c0b2",
      "245633e5f07348c4858e561f20edd250",
      "52a19fa3619741bcb5ad63b8ed7991c2",
      "da7a516f1703418e8306c53e63e57c67",
      "69d420f9849c4377af84f0335bb1fb67",
      "cca3e6167dda4b4e988b2159fc96e632",
      "cdb371acd43642ea9ce6b5058f9accf6",
      "86f541599d5444a59b46dc1b524bfc96",
      "34e5b01928e74300aa5e4e67eede2582",
      "a7aeeb1550ac4689b7533e20a297ae5f",
      "39bb89da8bb740d484e983aefb43847e",
      "79c7c9abe389448bbd102cc259e84d99",
      "abc5fd0d449f471bb5fa70ee5b75a5dd",
      "8b528a599aa443afa07615bdbdff29b6",
      "8533c31c11af41f5a6d6dec11d2252d2",
      "a4b7c52fb57c4d958036bdf787bfa992",
      "a86ac20169224fe7a831f8e15a2b95e5",
      "eb71aff28f1644669c0f1237453f88d5",
      "03fffcf4853d4e08875a894df04143ed",
      "48c0db5a29294223899a2c66d0757a7a",
      "2bdddb15ac464b418f1b8092a99e8dae",
      "7ff74887b60e446bb032309cb6412b64",
      "dc5913c3692d45e1b52969dbfbcf578d",
      "4bc8bba982954785afaf80292ff55ea4",
      "9b30f5f9922e4be8b8e551ff076018a1",
      "d80c7d8b078240c4a9e126f39e63c3d3",
      "a7642242ccb54e3abb8c34889087a22a",
      "3b762483c3574d09841ffd1b211eb1ea",
      "53756c5989d3401986e23e424399bbf5",
      "460e35948aef467f9c6e9817d1c3de2a",
      "8571765af6fb43239a3d2373d89dc086",
      "67e88295407f4e9a80638a4ec3dd15a5",
      "acaf0f7c8fe944df8a88b614df78d41c",
      "b56a406693e34a1ea624553118c66eca",
      "b241a3a057ca49e384b126752b149dda",
      "8db86661a8a84a869130a43b82885201",
      "0ceed16a37e748b8a7b62b2b4c988477",
      "b920db37163147cabe6973afb07383fb",
      "2a69a8bdafb74c27b86e0f6d8375b588",
      "bb67fe3c99884e8ea9dbd17a888f8710",
      "5f9bca619bae44779a0693517e77a5e6",
      "cc3e5ca0a7d84ee59a3fa2c675c9578d",
      "e1d0229a29164d41886d528e5bde9cdf",
      "0db0712cb88844a5b4ad7385e208ccec",
      "fd4daf32573048f9aed68704ede85247",
      "11a861ef82d74465a73873ec2b25d65e",
      "ba74d411e02d4026929db8f267abd8cf",
      "abaa106916ab42ed982950401269d55b",
      "8ae9e07ae1774364a3c21d17e5de48ea",
      "106434d534f34916ae8eb59ef70444d8",
      "1d3d67d32291480a84308fee4113db8c",
      "563852c880ae43b78f7b30b1ef1a84a3",
      "bb6e85b6656c4ce78b7b4f2bb7c720b4",
      "50893216d63c4571be1f937dbe9fd2af",
      "f643488d8d34450f842b9c31aa4f4b5a",
      "1eae4fb9531e455ba5388c7245a410e4",
      "195efa75330d4f8f9cc51dafea6b334a",
      "9b03bd8cc0504576ad7c5acb2c705e9c",
      "103d59c829024a7cb1538bbeb57ac7c1",
      "74da409f2b764ef3bee3aaf652395cd3",
      "690622decad44a45ac24490e77116a03",
      "27709ae17f02415cbd27e2ee348c0823",
      "55bc80a2464648f68c354022d3047528",
      "810da38712d64320be5dc75091024e2a",
      "6edc9e5a27d44ecca66e9ac382743a03",
      "7f806e512678424898edf5788422af5a",
      "1ed3db49e3854ed2b650be90ada6d833",
      "333df4de32854981b83cd1e4a68de73a",
      "bab8ea8859bf4344838e364136e6148f",
      "4e53af9a56154970acfa9dcc91f82308",
      "45f61527cd3544eebd9191bada768ba8",
      "64657780b739484dab0ca2fdd3135a30",
      "c75678cbcaed4cc7b0afb08211a4fae0",
      "61866d79e7e94d358d78c92be44dc309",
      "4183407a93644dd5aadcd691e002ec1d",
      "8e450302cbea492bb71c9d5911ab34b0",
      "b7cc3910288d40cf83f7c59c56725595",
      "81ad4f0f7a6044b7b71256a1ec92a452",
      "c0a414b4fb28467ca7a510635e6a8a34",
      "206ae4853fd64afd964e0c72d34c4bc5",
      "0b0e8e508e774fc98f2f3174d63d6c00",
      "56be08fb847642c687c5ca4b572fbcc1",
      "8c51270f30d94fb79d1003b2ce78a6b5",
      "88346aebda9c40ab917fd016d3912ac6",
      "fed771093e66461583981bf3645d7d94",
      "e6b369595304499aa0bb9ad76f92f4e1",
      "4942a6fda55a4dc583dabc8c31b81c67",
      "56b7d6c541c54a71bd24014d0e705932",
      "bb8c61aa08a54810bcf4b8b07f545920",
      "a6dc03634ed8432a87c5ea1f5a904dca",
      "69957f29415140948cfef21c3dd8f091",
      "fea72e2190f14e489cea4adec9f24bea",
      "a06bfb85ac8f4868881f8eb7d0be0cba",
      "0ee92c1ca31249c5ae1f5df2da6f553a",
      "48f5913398b14b6f82a5c8455146d45d",
      "6a2b4e56f14e46b5a40fb6488285e742",
      "33312921e0cd43ecb51b5ce48e298281",
      "073597c2a0df45f8afe055b22f6de37b",
      "23061cee5e514f2d83cb02bc774afe1e",
      "4409b7042d4c4931bb0fff9a24814368",
      "2076077e77f14c86add96fa860150e9c",
      "f654a78bd67747768ad89a90afe5f4c1",
      "d32fbbfe4ef2490b86944347eb9ca12a",
      "9457176970a5441a9464206750a93897",
      "ca24990d3c2c4894bdee7970fdc0c59f",
      "35ca0768e5514905895ddec65a31ec40",
      "c659f695efdb460fb55c0c2ae3bd077a",
      "0b093c77d1c24d60b2da6fcb625b6ab9",
      "1418a38552eb4389a786d00fcecd6943",
      "ea18af9d99894f5cb42f02ce06e9d364",
      "626a148d85c7403a9d7c1cce4898f69c",
      "6ba0be4867da48b2abece1940a6f0a09",
      "e28977d1e0864521b47731e6177197b0",
      "c711d9f9123f4d1a8bc0d6e35dd04ca4",
      "0e7884766b6d4c7a952c2b7363d83f85",
      "ad51586eec844ad19c96e7d2d806be2f",
      "1133802e533f411093b7d854398f8671",
      "e26f454da0da4e05b83de81664676b01",
      "fa1ebcce64ae4317b62a08ae8c65cac4",
      "8202abf3ca4040c994a706774c057c50",
      "f617d46c2a0349eb929a417393290caf",
      "9753742af2764f85a12a7a486c04829a",
      "18338935bdaf47e6b62490dc34f6df2b",
      "220ffe091ac3451fadab8f0b04a35696",
      "3f6c6c8099b0421d8f10c285069b0d9f",
      "db551c9985b948af93fb72ab956eafa2",
      "e1582afb96e2479ca4f3c9f85c7cf244",
      "44737e3376594bf696516e867744757f",
      "c8a7561f709f446f9f7e44b91ba34209",
      "906fbe2f88054b4f94e10a41f169a3be",
      "d885c22fdb8b49cdb17e5855d6672826",
      "185a5320568248eeade1258284fa9479",
      "1ce59990f0384b8aaa7158d8c5805ee3",
      "e25d4bdf50274eb89f497bc609dbac90",
      "ed1fdfd6c5814b7da6f2e8e3b5d08e0a",
      "14f8ffe99f5c481eabdd2aeffa8b2023",
      "a4d309a782384bdca257a6e6cf775f87",
      "f523142aeebd4a09a47a8f9698a97af7",
      "570dc9b0166e49cc90121e19d6f06a39",
      "da3cb6b5b3fb4a8681fa631666e8613b",
      "19bcbe7e586a43b797394aec74824552",
      "7bf638dd87ef4ef8a6b22b9513d816d4",
      "d07226a6ca5d430b9fb1c2e7c3d39618",
      "49087cf71f9346368129a36a8e4e5342",
      "6942157aa59a48a9a7809d1928e25892",
      "2c29609241a749a79fddd4ef616420c7"
     ]
    },
    "id": "sbYmhaqs0J59",
    "outputId": "fd760ace-40d3-4dc2-fd69-c97c2675c79e"
   },
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\" TESTING RAG CONFIGURATION\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# print(\"\\n Building FAISS index...\")\n",
    "# rag_config.build_index()\n",
    "# print(\" Index built successfully\")\n",
    "\n",
    "# # Check vector store\n",
    "# if hasattr(rag_config, 'retriever') and rag_config.retriever:\n",
    "#     if hasattr(rag_config.retriever, 'vectorstore'):\n",
    "#         vs = rag_config.retriever.vectorstore\n",
    "#         print(f\" Vector store contains {vs.index.ntotal} vectors\")\n",
    "\n",
    "# # Test retrieval for different question types\n",
    "# for q_type in ['Single-Doc RAG', 'Multi-Doc RAG']:\n",
    "#     type_queries = queries_df[queries_df['question_type'].str.contains(q_type, na=False)]\n",
    "\n",
    "#     if len(type_queries) == 0:\n",
    "#         continue\n",
    "\n",
    "#     test_query_id = type_queries.iloc[0]['query_id']\n",
    "#     test_query = type_queries.iloc[0]['query']\n",
    "\n",
    "#     print(f\"\\n Testing {q_type}:\")\n",
    "#     print(f\"   Query: '{test_query[:100]}...'\")\n",
    "\n",
    "#     test_result = rag_config.get_context(batch_queries=[test_query], serialize=False)\n",
    "\n",
    "#     if test_result and test_result[0]:\n",
    "#         print(f\"    Retrieved {len(test_result[0])} documents\")\n",
    "\n",
    "#         # Show top 3\n",
    "#         retrieved_sources = set()\n",
    "#         for i, doc in enumerate(test_result[0][:3]):\n",
    "#             corpus_id = doc.metadata.get('corpus_id', 'MISSING')\n",
    "#             source_file = doc.metadata.get('source_file', 'UNKNOWN')\n",
    "#             content_preview = doc.page_content[:100].replace('\\n', ' ')\n",
    "\n",
    "#             retrieved_sources.add(source_file)\n",
    "\n",
    "#             print(f\"\\n   Rank {i+1}:\")\n",
    "#             print(f\"      Doc ID: {corpus_id}\")\n",
    "#             print(f\"      Source: {source_file}\")\n",
    "#             print(f\"      Content: '{content_preview}...'\")\n",
    "\n",
    "#         # Check relevance\n",
    "#         retrieved_ids = [doc.metadata.get('corpus_id', '') for doc in test_result[0]]\n",
    "#         expected_docs = qrels_df[\n",
    "#             (qrels_df['query_id'] == test_query_id) &\n",
    "#             (qrels_df['relevance'] == 1)\n",
    "#         ]['corpus_id'].tolist()\n",
    "\n",
    "#         matches = set(retrieved_ids).intersection(set(expected_docs))\n",
    "\n",
    "#         # Get expected sources\n",
    "#         expected_sources = set()\n",
    "#         for doc_id in expected_docs[:10]:\n",
    "#             if doc_id in corpus_dict:\n",
    "#                 expected_sources.add(corpus_dict[doc_id][\"source_file\"])\n",
    "\n",
    "#         print(f\"\\n    Relevance Check:\")\n",
    "#         print(f\"      Expected relevant chunks: {len(expected_docs)}\")\n",
    "#         print(f\"      Expected sources: {', '.join(list(expected_sources)[:3])}\")\n",
    "#         print(f\"      Retrieved relevant chunks: {len(matches)}\")\n",
    "#         print(f\"      Retrieved sources: {', '.join(list(retrieved_sources))}\")\n",
    "#         print(f\"      Precision: {len(matches)}/{len(retrieved_ids)} = {len(matches)/len(retrieved_ids):.2%}\")\n",
    "\n",
    "#         if matches:\n",
    "#             print(f\"       Successfully retrieved relevant documents!\")\n",
    "#         else:\n",
    "#             print(f\"       No relevant documents in top results\")\n",
    "\n",
    "# print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "**Building FAISS index‚Ä¶**  \n",
    "\n",
    "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
    "The secret HF_TOKEN does not exist in your Colab secrets.\n",
    "To authenticate with the Hugging Face Hub, create a token in your settings tab\n",
    "(https://huggingface.co/settings/tokens), set it as secret in your Google Colab\n",
    "and restart your session.\n",
    "Authentication is recommended but optional for public models or datasets.\n",
    "\n",
    "\n",
    "**Index built successfully**  \n",
    "Vector store contains **10,355 vectors**\n",
    "\n",
    "---\n",
    "\n",
    "### Testing Multi-Doc RAG\n",
    "\n",
    "**Query:**  \n",
    "> *How has Apple's total net sales changed over time?*\n",
    "\n",
    "**Retrieved:** 5 documents\n",
    "\n",
    "**Rank 1**\n",
    "- **Doc ID:** doc_1733  \n",
    "- **Source:** 2023 Q2 AAPL  \n",
    "- **Content:** Services 20,907 ‚Üí 19,821 (+5%), Total net sales  \n",
    "  \\$94,836 ‚Üí \\$97,278 (-3%), \\$211,990 ‚Üí \\$221,2‚Ä¶\n",
    "\n",
    "**Rank 2**\n",
    "- **Doc ID:** doc_2042  \n",
    "- **Source:** 2023 Q3 AAPL  \n",
    "- **Content:** Lower net sales of Mac. Apple Inc. | Q3 2023 Form 10-Q | Page 19‚Ä¶\n",
    "\n",
    "**Rank 3**\n",
    "- **Doc ID:** doc_1732  \n",
    "- **Source:** 2023 Q2 AAPL  \n",
    "- **Content:** Higher net sales of iPhone, offset by lower Mac sales‚Ä¶\n",
    "\n",
    "---\n",
    "\n",
    "### Relevance Check\n",
    "\n",
    "- **Expected relevant chunks:** 423  \n",
    "- **Expected sources:** 2022 Q3 AAPL  \n",
    "- **Retrieved relevant chunks:** 5  \n",
    "- **Retrieved sources:** 2023 Q2 AAPL, 2023 Q3 AAPL  \n",
    "\n",
    "**Precision:**  \n",
    "> **5 / 5 = 100.00%** ‚úÖ  \n",
    "Successfully retrieved relevant documents!\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmtoJWa_4ZeA"
   },
   "source": [
    "Preprocess (retrieval-only focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmhtRJz93CzC"
   },
   "outputs": [],
   "source": [
    "def sample_preprocess_fn(\n",
    "    batch: Dict[str, listtype],\n",
    "    rag: RFLangChainRagSpec,\n",
    "    prompt_manager: RFPromptManager\n",
    ") -> Dict[str, listtype]:\n",
    "    \"\"\"Prepare inputs for the generator model\"\"\"\n",
    "\n",
    "    INSTRUCTIONS = (\n",
    "        \"You are a financial analyst assistant. Answer questions about tech company \"\n",
    "        \"financial performance based on quarterly 10-Q reports. Be precise, cite specific \"\n",
    "        \"figures, and reference the source documents when possible.\"\n",
    "    )\n",
    "\n",
    "    # Ensure queries are clean strings\n",
    "    batch_queries = [str(q).strip() for q in batch[\"query\"]]\n",
    "\n",
    "    # Perform retrieval\n",
    "    all_context = rag.get_context(batch_queries=batch_queries, serialize=False)\n",
    "\n",
    "    # Extract corpus IDs\n",
    "    retrieved_documents = [\n",
    "        [str(doc.metadata.get(\"corpus_id\", \"\")).strip() for doc in docs]\n",
    "        for docs in all_context\n",
    "    ]\n",
    "\n",
    "    # Serialize context\n",
    "    serialized_context = rag.serialize_documents(all_context)\n",
    "\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Financial Report Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "                },\n",
    "            ]\n",
    "            for question, context in zip(batch_queries, serialized_context)\n",
    "        ],\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "        **{k: list(v) for k, v in batch.items()},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m0b2Zfv4d5V"
   },
   "source": [
    "Postprocess (attach ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWoL5JZc20oy"
   },
   "outputs": [],
   "source": [
    "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
    "    \"\"\"Add ground truth documents to batch\"\"\"\n",
    "\n",
    "    gt_docs = []\n",
    "    for qid in batch[\"query_id\"]:\n",
    "        target_qid = str(qid).strip()\n",
    "        # Get only RELEVANT documents (relevance = 1)\n",
    "        relevant = qrels_df[\n",
    "            (qrels_df[\"query_id\"] == target_qid) &\n",
    "            (qrels_df[\"relevance\"] == 1)\n",
    "        ][\"corpus_id\"].tolist()\n",
    "        gt_docs.append(relevant)\n",
    "\n",
    "    batch[\"ground_truth_documents\"] = gt_docs\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U0dxiB64jVP"
   },
   "source": [
    "Metrics (Precision / Recall / MRR / NDCG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDycvu0K29vM"
   },
   "outputs": [],
   "source": [
    "def compute_ndcg_at_k(retrieved_docs, expected_docs, k=5):\n",
    "    \"\"\"Compute NDCG@k for ranked retrieval\"\"\"\n",
    "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
    "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "\n",
    "    ideal_length = min(k, len(expected_docs))\n",
    "    idcg = sum(1 / math.log2(i + 2) for i in range(ideal_length))\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Compute retrieval metrics per batch\"\"\"\n",
    "\n",
    "    precisions, recalls, f1s, ndcgs, rrs, hits = [], [], [], [], [], []\n",
    "    total_queries = len(batch[\"query\"])\n",
    "\n",
    "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
    "        # Ensure string type and strip\n",
    "        actual = set(str(p).strip() for p in pred)\n",
    "        expected = set(str(g).strip() for g in gt)\n",
    "\n",
    "        if not expected:\n",
    "            precisions.append(0)\n",
    "            recalls.append(0)\n",
    "            f1s.append(0)\n",
    "            ndcgs.append(0)\n",
    "            rrs.append(0)\n",
    "            hits.append(0)\n",
    "            continue\n",
    "\n",
    "        # True positives\n",
    "        tp = len(actual.intersection(expected))\n",
    "\n",
    "        # Metrics\n",
    "        precision = tp / len(actual) if actual else 0\n",
    "        precisions.append(precision)\n",
    "\n",
    "        recall = tp / len(expected) if expected else 0\n",
    "        recalls.append(recall)\n",
    "\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "        f1s.append(f1)\n",
    "\n",
    "        ndcg = compute_ndcg_at_k(pred, expected, k=8)  # k=8 since we retrieve 8\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "        hit = 1 if tp > 0 else 0\n",
    "        hits.append(hit)\n",
    "\n",
    "        rr = 0\n",
    "        for j, p in enumerate(pred):\n",
    "            if str(p).strip() in expected:\n",
    "                rr = 1 / (j + 1)\n",
    "                break\n",
    "        rrs.append(rr)\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        \"Hit_Rate\": {\"value\": sum(hits) / total_queries if total_queries > 0 else 0},\n",
    "        \"Precision\": {\"value\": sum(precisions) / total_queries if total_queries > 0 else 0},\n",
    "        \"Recall\": {\"value\": sum(recalls) / total_queries if total_queries > 0 else 0},\n",
    "        \"F1_Score\": {\"value\": sum(f1s) / total_queries if total_queries > 0 else 0},\n",
    "        \"NDCG@8\": {\"value\": sum(ndcgs) / total_queries if total_queries > 0 else 0},\n",
    "        \"MRR\": {\"value\": sum(rrs) / total_queries if total_queries > 0 else 0},\n",
    "    }\n",
    "\n",
    "\n",
    "def sample_accumulate_metrics_fn(\n",
    "    aggregated_metrics: Dict[str, listtype]\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Accumulate metrics across all batches\"\"\"\n",
    "\n",
    "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
    "    total_queries = sum(num_queries_per_batch)\n",
    "    metrics = [\"Hit_Rate\", \"Precision\", \"Recall\", \"F1_Score\", \"NDCG@8\", \"MRR\"]\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        **{\n",
    "            m: {\n",
    "                \"value\": sum(\n",
    "                    v[\"value\"] * queries\n",
    "                    for v, queries in zip(aggregated_metrics[m], num_queries_per_batch)\n",
    "                ) / total_queries if total_queries > 0 else 0,\n",
    "                \"is_algebraic\": True,\n",
    "                \"value_range\": (0, 1),\n",
    "            }\n",
    "            for m in metrics\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yir3wRg44zYn"
   },
   "source": [
    "vLLM Config + GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBoMhWDh3aww",
    "outputId": "2cd66c9c-8eb6-46f0-9328-20c2a4a28c39"
   },
   "outputs": [],
   "source": [
    "print(\" Configuring vLLM model...\")\n",
    "\n",
    "vllm_config = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.25,\n",
    "        \"enforce_eager\": True,\n",
    "        \"max_model_len\": 4096,  # Longer for multi-doc questions\n",
    "        \"disable_log_stats\": True,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 512,  # Longer answers for complex financial questions\n",
    "    },\n",
    "    rag=rag_config,\n",
    ")\n",
    "\n",
    "config_set = {\n",
    "    \"vllm_config\": vllm_config,\n",
    "    \"batch_size\": 3,  # Smaller batch for longer contexts\n",
    "    \"preprocess_fn\": sample_preprocess_fn,\n",
    "    \"postprocess_fn\": sample_postprocess_fn,\n",
    "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
    "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
    "    \"online_strategy_kwargs\": {\n",
    "        \"strategy_name\": \"normal\",\n",
    "        \"confidence_level\": 0.95,\n",
    "        \"use_fpc\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "config_group = RFGridSearch(config_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HKqqOff3Uj9"
   },
   "source": [
    "Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8851/dispatcher/get-pipeline-config-json/1": {
       "data": "eyJjb250ZXh0X2lkIjoxLCJwaXBlbGluZV9jb25maWdfanNvbiI6eyJiYXRjaF9zaXplIjozLCJtb2RlbF9jb25maWciOnsiZGlzYWJsZV9sb2dfc3RhdHMiOnRydWUsImRpc3RyaWJ1dGVkX2V4ZWN1dG9yX2JhY2tlbmQiOiJtcCIsImR0eXBlIjoiaGFsZiIsImVuZm9yY2VfZWFnZXIiOnRydWUsImdwdV9tZW1vcnlfdXRpbGl6YXRpb24iOjAuMjUsIm1heF9tb2RlbF9sZW4iOjQwOTYsIm1vZGVsIjoiUXdlbi9Rd2VuMi41LTAuNUItSW5zdHJ1Y3QiLCJ0ZW5zb3JfcGFyYWxsZWxfc2l6ZSI6MX0sIm9ubGluZV9zdHJhdGVneV9rd2FyZ3MiOnsiY29uZmlkZW5jZV9sZXZlbCI6MC45NSwic3RyYXRlZ3lfbmFtZSI6Im5vcm1hbCIsInVzZV9mcGMiOnRydWV9LCJwaXBlbGluZV90eXBlIjoidmxsbSIsInJhZ19jb25maWciOnsiY2h1bmtfb3ZlcmxhcCI6MTI4LCJjaHVua19zaXplIjo1MTIsImsiOjgsInNlYXJjaF90eXBlIjoic2ltaWxhcml0eSIsInRvcF9uIjo1fSwic2FtcGxpbmdfcGFyYW1zIjp7Im1heF90b2tlbnMiOjUxMiwidGVtcGVyYXR1cmUiOjAuNywidG9wX3AiOjAuOTV9fX0K",
       "headers": [
        [
         "content-length",
         "561"
        ],
        [
         "content-type",
         "application/json"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      },
      "http://localhost:8851/dispatcher/list-all-pipeline-ids": {
       "data": "W10K",
       "headers": [
        [
         "content-length",
         "3"
        ],
        [
         "content-type",
         "application/json"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "bZnVxy0p3Wdz",
    "outputId": "ce44fac9-f7e4-4ccb-c608-df99b37e9613"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ STARTING DOCUGAMI FINANCIAL QA BENCHMARK EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Dataset: {len(queries_df)} queries\")\n",
    "print(f\"   Corpus: {len(corpus_list)} document chunks\")\n",
    "print(f\"   QRELS: {len(qrels_df)} entries ({len(qrels_df[qrels_df['relevance']==1])} relevant)\")\n",
    "print(f\"   Question Types: {len(question_type_counts)} types\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "queries_dataset = Dataset.from_pandas(queries_df)\n",
    "\n",
    "# Create experiment\n",
    "experiment = Experiment(\n",
    "    experiment_name=\"docugami-financial-rag-benchmark\",\n",
    "    mode=\"evals\",\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "results = experiment.run_evals(\n",
    "    config_group=config_group,\n",
    "    dataset=queries_dataset,\n",
    "    num_actors=1,\n",
    "    num_shards=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzmJKUwR4_H8"
   },
   "source": [
    "Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtbSrHUQM0eO",
    "outputId": "ec296af1-044b-4681-cb72-99d5b62b9ffd"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DOCUGAMI FINANCIAL QA BENCHMARK EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            k: v['value'] if isinstance(v, dict) and 'value' in v else v\n",
    "            for k, v in {**metrics_dict, 'run_id': run_id}.items()\n",
    "        }\n",
    "        for run_id, (_, metrics_dict) in results.items()\n",
    "    ])\n",
    "\n",
    "    print(\"\\n OVERALL RESULTS:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n METRIC INTERPRETATION:\")\n",
    "    print(\"   ‚Ä¢ Hit Rate: % of queries where at least 1 relevant chunk was retrieved\")\n",
    "    print(\"   ‚Ä¢ Precision: Fraction of retrieved chunks that are relevant\")\n",
    "    print(\"   ‚Ä¢ Recall: Fraction of relevant chunks that were retrieved\")\n",
    "    print(\"   ‚Ä¢ F1 Score: Harmonic mean of precision and recall\")\n",
    "    print(\"   ‚Ä¢ NDCG@8: Ranking quality considering position of relevant chunks\")\n",
    "    print(\"   ‚Ä¢ MRR: Mean reciprocal rank of first relevant chunk\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" BENCHMARK CHARACTERISTICS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   ‚Ä¢ Multi-Document Support: Yes\")\n",
    "    print(f\"   ‚Ä¢ Multi-Chunk Support:  Yes\")\n",
    "    print(f\"   ‚Ä¢ Long-Form Documents:  Real 10-Q reports\")\n",
    "    print(f\"   ‚Ä¢ Varying Difficulty:  Single-doc and multi-doc questions\")\n",
    "    print(f\"   ‚Ä¢ Manual Curation:  Expert-reviewed answers\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"\\n No results returned from experiment\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO_7NyrDPxI6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
