{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPsY9pc0O1dA"
   },
   "source": [
    "RAG BENCHMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnTK-riOO-TC"
   },
   "source": [
    "Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOjnt3QbPTZZ",
    "outputId": "02c77776-e460-4973-c04a-d2d7b96d19d3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import rapidfireai\n",
    "    print(\" rapidfireai installed\")\n",
    "except ImportError:\n",
    "    !pip install rapidfireai datasets==3.6.0 langchain sentence-transformers PyPDF2\n",
    "    !rapidfireai init --evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3abpxyzXPDZh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List as listtype, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dataset and ML libraries\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# RapidFire components\n",
    "from rapidfireai import Experiment\n",
    "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9zf_Nb-Svrc"
   },
   "source": [
    "Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7qarDNsS4Rn",
    "outputId": "26e659c2-d2cd-450d-de47-a651f1d41e82"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üöÄ RAGBench Evaluation Pipeline - Complete Setup\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration\n",
    "DATASET_NAME = \"covidqa\"  # Options: covidqa, cuad, finqa, hotpotqa, msmarco, etc.\n",
    "DATASET_SPLIT = \"train\"\n",
    "OUTPUT_DIR = Path(\"./ragbench_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_NAME}\")\n",
    "print(f\"  Split: {DATASET_SPLIT}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üöÄ RAGBench Evaluation Pipeline - Complete Setup  \n",
    "================================================================================  \n",
    "\n",
    "üìã Configuration:  \n",
    "&nbsp;&nbsp;Dataset: covidqa  \n",
    "&nbsp;&nbsp;Split: train  \n",
    "&nbsp;&nbsp;Output Directory: ragbench_output  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16NIgGFPS5a5"
   },
   "source": [
    "Load RAG Bench Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlhhyGvZTAGj",
    "outputId": "299a4b05-3728-434d-aec2-4246d11aa436"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÇ LOADING RAGBENCH DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ragbench_dataset = load_dataset(\"rungalileo/ragbench\", DATASET_NAME, split=DATASET_SPLIT)\n",
    "\n",
    "print(f\"‚úì Loaded {len(ragbench_dataset)} samples from {DATASET_NAME}\")\n",
    "print(f\"\\nDataset features: {list(ragbench_dataset.features.keys())}\")\n",
    "\n",
    "# Inspect first sample\n",
    "sample = ragbench_dataset[0]\n",
    "print(f\"\\nüìã Sample 0 structure:\")\n",
    "print(f\"  - id: {sample['id']}\")\n",
    "print(f\"  - question: {sample['question'][:80]}...\")\n",
    "print(f\"  - documents: {len(sample['documents'])} documents\")\n",
    "print(f\"  - response: {sample['response'][:80]}...\")\n",
    "print(f\"  - documents_sentences: {len(sample['documents_sentences'])} document arrays\")\n",
    "print(f\"  - all_relevant_sentence_keys: {sample.get('all_relevant_sentence_keys', [])[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üìÇ LOADING RAGBENCH DATASET  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Loaded 1252 samples from covidqa  \n",
    "\n",
    "Dataset features: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score']  \n",
    "\n",
    "üìã Sample 0 structure:  \n",
    "&nbsp;&nbsp;- id: 358  \n",
    "&nbsp;&nbsp;- question: What role does T-cell count play in severe human adenovirus type 55 (HAdV-55) in...  \n",
    "&nbsp;&nbsp;- documents: 4 documents  \n",
    "&nbsp;&nbsp;- response: The T-cell count plays a crucial role in severe human adenovirus type 55 (HAdV-5...  \n",
    "&nbsp;&nbsp;- documents_sentences: 4 document arrays  \n",
    "&nbsp;&nbsp;- all_relevant_sentence_keys: ['0d', '0e', '0f', '1d', '1e']  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N6Y4Kg1TG-4"
   },
   "source": [
    "Create Corpus from RAG Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7UlbWjnTP8_",
    "outputId": "03fc2498-753b-4f14-d405-70bb185c776e"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî® CREATING CORPUS FROM RAGBENCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_corpus_from_ragbench(dataset):\n",
    "    \"\"\"\n",
    "    Convert RAGBench dataset to corpus format.\n",
    "\n",
    "    Creates sentence-level chunks that match ground truth annotations.\n",
    "    Each sentence gets a unique corpus_id and sent_key.\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset from RAGBench\n",
    "\n",
    "    Returns:\n",
    "        corpus_dict: Dictionary mapping corpus_id to chunk info\n",
    "        sample_to_corpus_mapping: Dictionary mapping sample_id to list of corpus_ids\n",
    "    \"\"\"\n",
    "    corpus_dict = {}\n",
    "    sample_to_corpus_mapping = {}\n",
    "\n",
    "    print(\"  Processing samples...\")\n",
    "\n",
    "    for sample_idx, sample in enumerate(dataset):\n",
    "        if sample_idx % 100 == 0:\n",
    "            print(f\"    Processed {sample_idx}/{len(dataset)} samples\")\n",
    "\n",
    "        sample_id = sample['id']\n",
    "        sample_corpus_ids = []\n",
    "\n",
    "        # Process each document in the sample\n",
    "        for doc_idx, doc_sentences in enumerate(sample['documents_sentences']):\n",
    "\n",
    "            # Create a chunk for each sentence\n",
    "            for sent_idx, sentence in enumerate(doc_sentences):\n",
    "                # Create unique corpus ID\n",
    "                corpus_id = f\"{sample_id}_doc{doc_idx}_sent{sent_idx}\"\n",
    "\n",
    "                # Create RAGBench-style sentence key: \"0a\", \"0b\", \"1a\", etc.\n",
    "                sent_key = f\"{doc_idx}{chr(97 + sent_idx)}\"\n",
    "\n",
    "                corpus_dict[corpus_id] = {\n",
    "                    \"text\": sentence,\n",
    "                    \"sample_id\": sample_id,\n",
    "                    \"doc_index\": doc_idx,\n",
    "                    \"sent_index\": sent_idx,\n",
    "                    \"sent_key\": sent_key,\n",
    "                    \"_id\": corpus_id,  # Add _id field for JSONLoader compatibility\n",
    "                }\n",
    "\n",
    "                sample_corpus_ids.append(corpus_id)\n",
    "\n",
    "        sample_to_corpus_mapping[sample_id] = sample_corpus_ids\n",
    "\n",
    "    print(f\"  ‚úì Processed {len(dataset)} samples\")\n",
    "    return corpus_dict, sample_to_corpus_mapping\n",
    "\n",
    "corpus_dict, sample_mapping = create_corpus_from_ragbench(ragbench_dataset)\n",
    "\n",
    "print(f\"\\n‚úì Created corpus with {len(corpus_dict)} sentence-level chunks\")\n",
    "print(f\"  From {len(ragbench_dataset)} samples\")\n",
    "print(f\"  Average chunks per sample: {len(corpus_dict) / len(ragbench_dataset):.1f}\")\n",
    "\n",
    "# Show statistics\n",
    "sentence_lengths = [len(chunk['text']) for chunk in corpus_dict.values()]\n",
    "print(f\"\\nChunk statistics:\")\n",
    "print(f\"  Mean length: {sum(sentence_lengths) / len(sentence_lengths):.1f} chars\")\n",
    "print(f\"  Min length: {min(sentence_lengths)} chars\")\n",
    "print(f\"  Max length: {max(sentence_lengths)} chars\")\n",
    "\n",
    "# Show example\n",
    "example_corpus_id = list(corpus_dict.keys())[0]\n",
    "example_chunk = corpus_dict[example_corpus_id]\n",
    "print(f\"\\nüìã Example corpus entry:\")\n",
    "print(f\"  Corpus ID: {example_corpus_id}\")\n",
    "print(f\"  Sent Key: {example_chunk['sent_key']}\")\n",
    "print(f\"  Text: {example_chunk['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üî® CREATING CORPUS FROM RAGBENCH  \n",
    "================================================================================  \n",
    "\n",
    "&nbsp;&nbsp;Processing samples...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 0/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 100/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 200/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 300/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 400/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 500/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 600/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 700/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 800/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 900/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1000/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1100/1252 samples  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1200/1252 samples  \n",
    "&nbsp;&nbsp;‚úì Processed 1252 samples  \n",
    "\n",
    "‚úì Created corpus with 23023 sentence-level chunks  \n",
    "&nbsp;&nbsp;From 1252 samples  \n",
    "&nbsp;&nbsp;Average chunks per sample: 18.4  \n",
    "\n",
    "Chunk statistics:  \n",
    "&nbsp;&nbsp;Mean length: 2.0 chars  \n",
    "&nbsp;&nbsp;Min length: 2 chars  \n",
    "&nbsp;&nbsp;Max length: 2 chars  \n",
    "\n",
    "üìã Example corpus entry:  \n",
    "&nbsp;&nbsp;Corpus ID: 358_doc0_sent0  \n",
    "&nbsp;&nbsp;Sent Key: 0a  \n",
    "&nbsp;&nbsp;Text: ['0a', 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study']...  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brQOpYFjTR_N"
   },
   "source": [
    "Save corpus to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgiky9C1TW34",
    "outputId": "b936160f-6527-46d6-8926-a77e7c730f5c"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SAVING CORPUS TO JSONL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corpus_file = OUTPUT_DIR / \"corpus.jsonl\"\n",
    "\n",
    "with open(corpus_file, 'w') as f:\n",
    "    for corpus_id, chunk_info in corpus_dict.items():\n",
    "        json.dump(chunk_info, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úì Saved {len(corpus_dict)} corpus entries to {corpus_file}\")\n",
    "\n",
    "# Verify file\n",
    "with open(corpus_file, 'r') as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(f\"\\n  First line verification:\")\n",
    "    print(f\"    _id: {first_line.get('_id')}\")\n",
    "    print(f\"    text: {first_line.get('text')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üíæ SAVING CORPUS TO JSONL  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Saved 23023 corpus entries to ragbench_output/corpus.jsonl  \n",
    "\n",
    "&nbsp;&nbsp;First line verification:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;_id: 358_doc0_sent0  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;text: ['0a', 'Title: Emergent severe acute respiratory distress syndrome caused by adenovirus type 55 in immunocompetent adults in 2013: a prospective observational study']...  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDegEiK3TYPO"
   },
   "source": [
    "Create Queries and Qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOU32rMsTe7F",
    "outputId": "0852e98c-c98b-49d5-e091-1b6e80e56ac7"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî® CREATING QUERIES AND QRELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def create_queries_and_qrels(dataset, corpus_dict, sample_mapping):\n",
    "    \"\"\"\n",
    "    Create queries and relevance judgments from RAGBench.\n",
    "\n",
    "    Uses the 'all_relevant_sentence_keys' field to identify relevant chunks.\n",
    "    Also adds some negative samples for better evaluation.\n",
    "\n",
    "    Args:\n",
    "        dataset: HuggingFace Dataset from RAGBench\n",
    "        corpus_dict: Corpus dictionary\n",
    "        sample_mapping: Sample to corpus ID mapping\n",
    "\n",
    "    Returns:\n",
    "        queries_list: List of query dictionaries\n",
    "        qrels_rows: List of QRELS entries\n",
    "    \"\"\"\n",
    "    queries_list = []\n",
    "    qrels_rows = []\n",
    "\n",
    "    print(\"  Processing queries and QRELS...\")\n",
    "\n",
    "    for sample_idx, sample in enumerate(dataset):\n",
    "        if sample_idx % 100 == 0:\n",
    "            print(f\"    Processed {sample_idx}/{len(dataset)} queries\")\n",
    "\n",
    "        sample_id = sample['id']\n",
    "        question = sample['question']\n",
    "\n",
    "        # Add query\n",
    "        queries_list.append({\n",
    "            \"query_id\": sample_id,\n",
    "            \"query\": question,\n",
    "        })\n",
    "\n",
    "        # Get relevant sentence keys (e.g., [\"0a\", \"0d\", \"1b\"])\n",
    "        relevant_keys = sample.get('all_relevant_sentence_keys', [])\n",
    "\n",
    "        # Map sentence keys to corpus IDs for this sample\n",
    "        sample_corpus_ids = sample_mapping[sample_id]\n",
    "\n",
    "        relevant_corpus_ids = []\n",
    "        irrelevant_corpus_ids = []\n",
    "\n",
    "        for corpus_id in sample_corpus_ids:\n",
    "            chunk_info = corpus_dict[corpus_id]\n",
    "            sent_key = chunk_info['sent_key']\n",
    "\n",
    "            # Check if this sentence is relevant\n",
    "            is_relevant = sent_key in relevant_keys\n",
    "\n",
    "            if is_relevant:\n",
    "                relevant_corpus_ids.append(corpus_id)\n",
    "            else:\n",
    "                irrelevant_corpus_ids.append(corpus_id)\n",
    "\n",
    "            # Add to QRELS\n",
    "            qrels_rows.append({\n",
    "                \"query_id\": sample_id,\n",
    "                \"corpus_id\": corpus_id,\n",
    "                \"relevance\": 1 if is_relevant else 0\n",
    "            })\n",
    "\n",
    "    print(f\"  ‚úì Processed {len(dataset)} queries\")\n",
    "    return queries_list, qrels_rows\n",
    "\n",
    "queries_list, qrels_rows = create_queries_and_qrels(\n",
    "    ragbench_dataset,\n",
    "    corpus_dict,\n",
    "    sample_mapping\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Created {len(queries_list)} queries\")\n",
    "print(f\"‚úì Created {len(qrels_rows)} QRELS entries\")\n",
    "\n",
    "relevant_count = sum(1 for q in qrels_rows if q['relevance'] == 1)\n",
    "irrelevant_count = len(qrels_rows) - relevant_count\n",
    "\n",
    "print(f\"\\n  Class distribution:\")\n",
    "print(f\"    Relevant: {relevant_count} ({100*relevant_count/len(qrels_rows):.1f}%)\")\n",
    "print(f\"    Irrelevant: {irrelevant_count} ({100*irrelevant_count/len(qrels_rows):.1f}%)\")\n",
    "\n",
    "# Calculate average relevant docs per query\n",
    "relevant_per_query = defaultdict(int)\n",
    "for qrel in qrels_rows:\n",
    "    if qrel['relevance'] == 1:\n",
    "        relevant_per_query[qrel['query_id']] += 1\n",
    "\n",
    "avg_relevant = sum(relevant_per_query.values()) / len(relevant_per_query)\n",
    "print(f\"\\n  Average relevant sentences per query: {avg_relevant:.1f}\")\n",
    "print(f\"  Min relevant: {min(relevant_per_query.values())}\")\n",
    "print(f\"  Max relevant: {max(relevant_per_query.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üî® CREATING QUERIES AND QRELS  \n",
    "================================================================================  \n",
    "\n",
    "&nbsp;&nbsp;Processing queries and QRELS...  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 0/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 100/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 200/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 300/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 400/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 500/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 600/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 700/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 800/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 900/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1000/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1100/1252 queries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Processed 1200/1252 queries  \n",
    "&nbsp;&nbsp;‚úì Processed 1252 queries  \n",
    "\n",
    "‚úì Created 1252 queries  \n",
    "‚úì Created 23023 QRELS entries  \n",
    "\n",
    "&nbsp;&nbsp;Class distribution:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Relevant: 6422 (27.9%)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Irrelevant: 16601 (72.1%)  \n",
    "\n",
    "&nbsp;&nbsp;Average relevant sentences per query: 5.3  \n",
    "&nbsp;&nbsp;Min relevant: 1  \n",
    "&nbsp;&nbsp;Max relevant: 20  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EYRqEeJTgaw"
   },
   "source": [
    "Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_NoJV86TlpG",
    "outputId": "4cc6eca4-b904-4001-d5d0-89ba8a7eaec4"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CREATING DATAFRAMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create queries DataFrame\n",
    "queries_df = pd.DataFrame(queries_list)\n",
    "queries_df['query_id'] = queries_df['query_id'].astype(str).str.strip()\n",
    "queries_df['query'] = queries_df['query'].astype(str).str.strip()\n",
    "\n",
    "print(f\"‚úì Queries DataFrame: {len(queries_df)} rows\")\n",
    "print(f\"  Columns: {queries_df.columns.tolist()}\")\n",
    "\n",
    "# Create QRELS DataFrame\n",
    "qrels_df = pd.DataFrame(qrels_rows)\n",
    "qrels_df['query_id'] = qrels_df['query_id'].astype(str).str.strip()\n",
    "qrels_df['corpus_id'] = qrels_df['corpus_id'].astype(str).str.strip()\n",
    "qrels_df['relevance'] = qrels_df['relevance'].astype(int)\n",
    "\n",
    "print(f\"‚úì QRELS DataFrame: {len(qrels_df)} rows\")\n",
    "print(f\"  Columns: {qrels_df.columns.tolist()}\")\n",
    "\n",
    "# Save DataFrames for reference\n",
    "queries_df.to_csv(OUTPUT_DIR / \"queries.csv\", index=False)\n",
    "qrels_df.to_csv(OUTPUT_DIR / \"qrels.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved queries and QRELS to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üìä CREATING DATAFRAMES  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Queries DataFrame: 1252 rows  \n",
    "&nbsp;&nbsp;Columns: ['query_id', 'query']  \n",
    "\n",
    "‚úì QRELS DataFrame: 23023 rows  \n",
    "&nbsp;&nbsp;Columns: ['query_id', 'corpus_id', 'relevance']  \n",
    "\n",
    "‚úì Saved queries and QRELS to ragbench_output  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Rortz4ATm5N"
   },
   "source": [
    "Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kQoI0lzTqbc",
    "outputId": "28fe8a4c-f3f7-42fd-893d-bdeddc7dad20"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ VALIDATION CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checks = [\n",
    "    (\"Corpus not empty\", len(corpus_dict) > 0),\n",
    "    (\"Queries not empty\", len(queries_df) > 0),\n",
    "    (\"QRELS not empty\", len(qrels_df) > 0),\n",
    "    (\"All query_ids in QRELS exist in queries\",\n",
    "     set(qrels_df['query_id'].unique()).issubset(set(queries_df['query_id']))),\n",
    "    (\"All corpus_ids in QRELS exist in corpus\",\n",
    "     set(qrels_df['corpus_id'].unique()).issubset(set(corpus_dict.keys()))),\n",
    "    (\"Class balance reasonable (10-90% relevant)\",\n",
    "     0.10 < relevant_count/len(qrels_rows) < 0.90),\n",
    "    (\"Corpus file exists\", corpus_file.exists()),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for check_name, result in checks:\n",
    "    status = \"‚úì\" if result else \"‚úó\"\n",
    "    print(f\"  {status} {check_name}\")\n",
    "    if not result:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n‚úÖ All validation checks passed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some checks failed - review before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### ‚úÖ VALIDATION CHECKS  \n",
    "================================================================================  \n",
    "\n",
    "&nbsp;&nbsp;‚úì Corpus not empty  \n",
    "&nbsp;&nbsp;‚úì Queries not empty  \n",
    "&nbsp;&nbsp;‚úì QRELS not empty  \n",
    "&nbsp;&nbsp;‚úì All query_ids in QRELS exist in queries  \n",
    "&nbsp;&nbsp;‚úì All corpus_ids in QRELS exist in corpus  \n",
    "&nbsp;&nbsp;‚úì Class balance reasonable (10-90% relevant)  \n",
    "&nbsp;&nbsp;‚úì Corpus file exists  \n",
    "\n",
    "‚úÖ All validation checks passed!  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI3liGCNTr0j"
   },
   "source": [
    "RAG Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DJ4pAYzTx-o",
    "outputId": "8a2f0d53-9f06-49ab-8909-c80f01dba712"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öôÔ∏è  CONFIGURING RAG PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "rag_config = RFLangChainRagSpec(\n",
    "    document_loader=DirectoryLoader(\n",
    "        path=str(OUTPUT_DIR),\n",
    "        glob=\"corpus.jsonl\",\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"metadata_func\": lambda record, metadata: {\n",
    "                \"corpus_id\": str(record.get(\"_id\", \"\")).strip(),\n",
    "                \"sample_id\": str(record.get(\"sample_id\", \"\")).strip(),\n",
    "                \"sent_key\": str(record.get(\"sent_key\", \"\")).strip(),\n",
    "                \"doc_index\": record.get(\"doc_index\", 0),\n",
    "                \"sent_index\": record.get(\"sent_index\", 0),\n",
    "            },\n",
    "            \"json_lines\": True,\n",
    "            \"text_content\": False,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # CRITICAL: Use large chunk_size to prevent re-chunking sentences\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=100000,  # Much larger than sentence length\n",
    "        chunk_overlap=0,\n",
    "    ),\n",
    "\n",
    "    embedding_cls=HuggingFaceEmbeddings,\n",
    "    embedding_kwargs={\n",
    "        \"model_name\": \"BAAI/bge-base-en-v1.5\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "        \"encode_kwargs\": {\n",
    "            \"normalize_embeddings\": True,\n",
    "            \"batch_size\": batch_size\n",
    "        },\n",
    "    },\n",
    "\n",
    "    vector_store=None,\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10},  # Retrieve top-10 sentences\n",
    "\n",
    "    # Optional: Add reranker for better results\n",
    "    reranker_cls=CrossEncoderReranker,\n",
    "    reranker_kwargs={\n",
    "        \"model_name\": \"BAAI/bge-reranker-base\",\n",
    "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
    "        \"top_n\": 5,  # Keep top 5 after reranking\n",
    "    },\n",
    "\n",
    "    enable_gpu_search=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úì RAG configuration created\")\n",
    "print(f\"  Embedding model: BAAI/bge-base-en-v1.5\")\n",
    "print(f\"  Retrieval: top-5 similarity search\")\n",
    "print(f\"  Reranker: BAAI/bge-reranker-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### ‚öôÔ∏è  CONFIGURING RAG PIPELINE  \n",
    "================================================================================  \n",
    "\n",
    "‚úì RAG configuration created  \n",
    "&nbsp;&nbsp;Embedding model: BAAI/bge-base-en-v1.5  \n",
    "&nbsp;&nbsp;Retrieval: top-5 similarity search  \n",
    "&nbsp;&nbsp;Reranker: BAAI/bge-reranker-base  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHtQCtRQTzix"
   },
   "source": [
    "Preprocessing and Postprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwQ7cjzrT8Jq",
    "outputId": "236fabba-971f-4553-a1ec-9234ffd6e2b5"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîß DEFINING PREPROCESSING AND POSTPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_fn(\n",
    "    batch: Dict[str, List],\n",
    "    rag: RFLangChainRagSpec,\n",
    "    prompt_manager: RFPromptManager\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Prepare inputs for the generator model.\n",
    "\n",
    "    This function:\n",
    "    1. Retrieves relevant documents for each query\n",
    "    2. Extracts corpus IDs from retrieved documents\n",
    "    3. Formats prompts for the LLM\n",
    "\n",
    "    Args:\n",
    "        batch: Batch of queries\n",
    "        rag: RAG specification\n",
    "        prompt_manager: Prompt manager (unused here)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with prompts and retrieved documents\n",
    "    \"\"\"\n",
    "    INSTRUCTIONS = (\n",
    "        f\"You are an expert assistant for {DATASET_NAME} questions. \"\n",
    "        \"Answer questions based on the provided context documents. \"\n",
    "        \"Be precise, cite specific information, and reference the source when possible.\"\n",
    "    )\n",
    "\n",
    "    # Ensure queries are clean strings\n",
    "    batch_queries = [str(q).strip() for q in batch[\"query\"]]\n",
    "\n",
    "    # Perform retrieval\n",
    "    all_context = rag.get_context(batch_queries=batch_queries, serialize=False)\n",
    "\n",
    "    # Extract corpus IDs from retrieved documents\n",
    "    retrieved_documents = [\n",
    "        [str(doc.metadata.get(\"corpus_id\", \"\")).strip() for doc in docs]\n",
    "        for docs in all_context\n",
    "    ]\n",
    "\n",
    "    # Serialize context for LLM\n",
    "    serialized_context = rag.serialize_documents(all_context)\n",
    "\n",
    "    return {\n",
    "        \"prompts\": [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Context Documents:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "                },\n",
    "            ]\n",
    "            for question, context in zip(batch_queries, serialized_context)\n",
    "        ],\n",
    "        \"retrieved_documents\": retrieved_documents,\n",
    "        **{k: list(v) for k, v in batch.items()},\n",
    "    }\n",
    "\n",
    "def postprocess_fn(batch: Dict[str, List]) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Add ground truth documents to batch for evaluation.\n",
    "\n",
    "    Args:\n",
    "        batch: Batch with retrieved documents\n",
    "\n",
    "    Returns:\n",
    "        Batch with ground_truth_documents added\n",
    "    \"\"\"\n",
    "    gt_docs = []\n",
    "\n",
    "    for qid in batch[\"query_id\"]:\n",
    "        target_qid = str(qid).strip()\n",
    "        # Get only RELEVANT documents (relevance = 1) from QRELS\n",
    "        relevant = qrels_df[\n",
    "            (qrels_df[\"query_id\"] == target_qid) &\n",
    "            (qrels_df[\"relevance\"] == 1)\n",
    "        ][\"corpus_id\"].tolist()\n",
    "        gt_docs.append(relevant)\n",
    "\n",
    "    batch[\"ground_truth_documents\"] = gt_docs\n",
    "    return batch\n",
    "\n",
    "print(\"‚úì Preprocessing function defined\")\n",
    "print(\"‚úì Postprocessing function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üîß DEFINING PREPROCESSING AND POSTPROCESSING  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Preprocessing function defined  \n",
    "‚úì Postprocessing function defined  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8QwgS1hT9L6"
   },
   "source": [
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EdkWau_UDhO",
    "outputId": "ef6e5a07-c558-40c4-b7dd-f444a1fb7cba"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìè DEFINING EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compute_metrics_fn(batch: Dict[str, List]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute RAG evaluation metrics.\n",
    "\n",
    "    Metrics:\n",
    "    - Precision: Fraction of retrieved chunks that are relevant\n",
    "    - Recall: Fraction of relevant chunks that were retrieved\n",
    "    - F1: Harmonic mean of precision and recall\n",
    "    - NDCG@k: Normalized discounted cumulative gain\n",
    "    - MRR: Mean reciprocal rank\n",
    "    - Hit Rate: At least one relevant chunk retrieved\n",
    "\n",
    "    Args:\n",
    "        batch: Batch with retrieved_documents and ground_truth_documents\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of computed metrics\n",
    "    \"\"\"\n",
    "    precisions, recalls, f1s, ndcgs, mrrs, hits = [], [], [], [], [], []\n",
    "\n",
    "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
    "        # Convert to sets of strings\n",
    "        predicted = set(str(p).strip() for p in pred)\n",
    "        expected = set(str(g).strip() for g in gt)\n",
    "\n",
    "        if not expected:\n",
    "            # Skip queries with no ground truth\n",
    "            continue\n",
    "\n",
    "        # Calculate overlap\n",
    "        tp = len(predicted & expected)\n",
    "        fp = len(predicted - expected)\n",
    "        fn = len(expected - predicted)\n",
    "\n",
    "        # Metrics\n",
    "        precision = tp / len(predicted) if predicted else 0\n",
    "        recall = tp / len(expected) if expected else 0\n",
    "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "        # Hit rate: Did we retrieve at least one relevant document?\n",
    "        hits.append(1 if tp > 0 else 0)\n",
    "\n",
    "        # MRR: Mean Reciprocal Rank\n",
    "        rr = 0\n",
    "        for j, p in enumerate(pred):\n",
    "            if str(p).strip() in expected:\n",
    "                rr = 1 / (j + 1)\n",
    "                break\n",
    "        mrrs.append(rr)\n",
    "\n",
    "        # NDCG@k\n",
    "        k = len(pred)\n",
    "        relevance = [1 if str(doc).strip() in expected else 0 for doc in pred[:k]]\n",
    "        dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "        idcg = sum(1 / math.log2(i + 2) for i in range(min(k, len(expected))))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    total = len(batch[\"query\"])\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total},\n",
    "        \"Precision\": {\"value\": sum(precisions) / total if total > 0 else 0},\n",
    "        \"Recall\": {\"value\": sum(recalls) / total if total > 0 else 0},\n",
    "        \"F1_Score\": {\"value\": sum(f1s) / total if total > 0 else 0},\n",
    "        \"NDCG@k\": {\"value\": sum(ndcgs) / total if total > 0 else 0},\n",
    "        \"MRR\": {\"value\": sum(mrrs) / total if total > 0 else 0},\n",
    "        \"Hit_Rate\": {\"value\": sum(hits) / total if total > 0 else 0},\n",
    "    }\n",
    "\n",
    "def accumulate_metrics_fn(\n",
    "    aggregated_metrics: Dict[str, List]\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Accumulate metrics across all batches.\n",
    "\n",
    "    Args:\n",
    "        aggregated_metrics: Metrics from all batches\n",
    "\n",
    "    Returns:\n",
    "        Final aggregated metrics\n",
    "    \"\"\"\n",
    "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
    "    total_queries = sum(num_queries_per_batch)\n",
    "    metrics = [\"Hit_Rate\", \"Precision\", \"Recall\", \"F1_Score\", \"NDCG@k\", \"MRR\"]\n",
    "\n",
    "    return {\n",
    "        \"Total\": {\"value\": total_queries},\n",
    "        **{\n",
    "            m: {\n",
    "                \"value\": sum(\n",
    "                    v[\"value\"] * queries\n",
    "                    for v, queries in zip(aggregated_metrics[m], num_queries_per_batch)\n",
    "                ) / total_queries if total_queries > 0 else 0,\n",
    "                \"is_algebraic\": True,\n",
    "                \"value_range\": (0, 1),\n",
    "            }\n",
    "            for m in metrics\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úì Metrics functions defined\")\n",
    "print(\"  Metrics: Precision, Recall, F1, NDCG@k, MRR, Hit Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üìè DEFINING EVALUATION METRICS  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Metrics functions defined  \n",
    "&nbsp;&nbsp;Metrics: Precision, Recall, F1, NDCG@k, MRR, Hit Rate  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbq3BxgJUEs9"
   },
   "source": [
    "vLLM Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Lv8Jz9eUI13",
    "outputId": "3705f3f4-74d6-49fc-de1a-80e90e9cf552"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ CONFIGURING vLLM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vllm_config = RFvLLMModelConfig(\n",
    "    model_config={\n",
    "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "        \"dtype\": \"half\",\n",
    "        \"gpu_memory_utilization\": 0.3,\n",
    "        \"enforce_eager\": True,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"distributed_executor_backend\": \"mp\",\n",
    "    },\n",
    "    sampling_params={\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 256,  # Reasonable length for answers\n",
    "    },\n",
    "    rag=rag_config,\n",
    ")\n",
    "\n",
    "print(\"‚úì vLLM configuration created\")\n",
    "print(f\"  Model: Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "print(f\"  Max tokens: 256\")\n",
    "print(f\"  Temperature: 0.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### ü§ñ CONFIGURING vLLM MODEL  \n",
    "================================================================================  \n",
    "\n",
    "‚úì vLLM configuration created  \n",
    "&nbsp;&nbsp;Model: Qwen/Qwen2.5-0.5B-Instruct  \n",
    "&nbsp;&nbsp;Max tokens: 256  \n",
    "&nbsp;&nbsp;Temperature: 0.7  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADSGmi-KUKsW"
   },
   "source": [
    "Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymtW8toNUPL1",
    "outputId": "dac652a4-f208-4200-f6fc-30a3b4fc72c3"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî¨ CONFIGURING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_set = {\n",
    "    \"vllm_config\": vllm_config,\n",
    "    \"batch_size\": 4,  # Process 4 queries at a time\n",
    "    \"preprocess_fn\": preprocess_fn,\n",
    "    \"postprocess_fn\": postprocess_fn,\n",
    "    \"compute_metrics_fn\": compute_metrics_fn,\n",
    "    \"accumulate_metrics_fn\": accumulate_metrics_fn,\n",
    "    \"online_strategy_kwargs\": {\n",
    "        \"strategy_name\": \"normal\",\n",
    "        \"confidence_level\": 0.95,\n",
    "        \"use_fpc\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "config_group = RFGridSearch(config_set)\n",
    "\n",
    "print(\"‚úì Experiment configuration created\")\n",
    "print(f\"  Batch size: 4\")\n",
    "print(f\"  Confidence level: 95%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üî¨ CONFIGURING EXPERIMENT  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Experiment configuration created  \n",
    "&nbsp;&nbsp;Batch size: 4  \n",
    "&nbsp;&nbsp;Confidence level: 95%  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR-NlxPPURrY"
   },
   "source": [
    "Convert to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDzJjr_oUWsF",
    "outputId": "1d25d924-1e5a-4f51-a07f-2211a4811830"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ PREPARING DATASET FOR EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "queries_dataset = Dataset.from_pandas(queries_df)\n",
    "\n",
    "print(f\"‚úì Created HuggingFace Dataset with {len(queries_dataset)} queries\")\n",
    "print(f\"  Features: {list(queries_dataset.features.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üì¶ PREPARING DATASET FOR EVALUATION  \n",
    "================================================================================  \n",
    "\n",
    "‚úì Created HuggingFace Dataset with 1252 queries  \n",
    "&nbsp;&nbsp;Features: ['query_id', 'query']  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GE5HQAtUadp"
   },
   "source": [
    "Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8851/dispatcher/get-pipeline-config-json/1": {
       "data": "eyJjb250ZXh0X2lkIjoxLCJwaXBlbGluZV9jb25maWdfanNvbiI6eyJiYXRjaF9zaXplIjo0LCJtb2RlbF9jb25maWciOnsiZGlzYWJsZV9sb2dfc3RhdHMiOnRydWUsImRpc3RyaWJ1dGVkX2V4ZWN1dG9yX2JhY2tlbmQiOiJtcCIsImR0eXBlIjoiaGFsZiIsImVuZm9yY2VfZWFnZXIiOnRydWUsImdwdV9tZW1vcnlfdXRpbGl6YXRpb24iOjAuMywibWF4X21vZGVsX2xlbiI6NDA5NiwibW9kZWwiOiJRd2VuL1F3ZW4yLjUtMC41Qi1JbnN0cnVjdCIsInRlbnNvcl9wYXJhbGxlbF9zaXplIjoxfSwib25saW5lX3N0cmF0ZWd5X2t3YXJncyI6eyJjb25maWRlbmNlX2xldmVsIjowLjk1LCJzdHJhdGVneV9uYW1lIjoibm9ybWFsIiwidXNlX2ZwYyI6dHJ1ZX0sInBpcGVsaW5lX3R5cGUiOiJ2bGxtIiwicmFnX2NvbmZpZyI6eyJjaHVua19vdmVybGFwIjowLCJjaHVua19zaXplIjoxMDAwMDAsImsiOjEwLCJzZWFyY2hfdHlwZSI6InNpbWlsYXJpdHkiLCJ0b3BfbiI6NX0sInNhbXBsaW5nX3BhcmFtcyI6eyJtYXhfdG9rZW5zIjoyNTYsInRlbXBlcmF0dXJlIjowLjcsInRvcF9wIjowLjk1fX19Cg==",
       "headers": [
        [
         "content-length",
         "562"
        ],
        [
         "content-type",
         "application/json"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      },
      "http://localhost:8851/dispatcher/list-all-pipeline-ids": {
       "data": "W3sicGlwZWxpbmVfaWQiOjEsInNoYXJkc19jb21wbGV0ZWQiOjAsInN0YXR1cyI6Im9uZ29pbmciLCJ0b3RhbF9zYW1wbGVzX3Byb2Nlc3NlZCI6MH1dCg==",
       "headers": [
        [
         "content-length",
         "88"
        ],
        [
         "content-type",
         "application/json"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "r_hEFreTUfRa",
    "outputId": "b36317f3-facd-485a-8eb4-33725db0e7d5"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING RAGBench EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Evaluation Summary:\")\n",
    "print(f\"  Dataset: {DATASET_NAME} ({DATASET_SPLIT} split)\")\n",
    "print(f\"  Queries: {len(queries_dataset)}\")\n",
    "print(f\"  Corpus: {len(corpus_dict)} sentence-level chunks\")\n",
    "print(f\"  QRELS entries: {len(qrels_df)}\")\n",
    "print(f\"  Relevant entries: {relevant_count} ({100*relevant_count/len(qrels_rows):.1f}%)\")\n",
    "print(f\"  Model: Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create experiment\n",
    "experiment = Experiment(\n",
    "    experiment_name=f\"ragbench-{DATASET_NAME}-evaluation\",\n",
    "    mode=\"evals\",\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"üèÉ Running evaluation... (this may take several minutes)\\n\")\n",
    "\n",
    "results = experiment.run_evals(\n",
    "    config_group=config_group,\n",
    "    dataset=queries_dataset,\n",
    "    num_actors=1,\n",
    "    num_shards=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# End experiment\n",
    "experiment.end()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "================================================================================  \n",
    "#### üöÄ STARTING RAGBench EVALUATION  \n",
    "================================================================================  \n",
    "\n",
    "#### üìä Evaluation Summary:  \n",
    "#### &nbsp;&nbsp;Dataset: covidqa (train split)  \n",
    "#### &nbsp;&nbsp;Queries: 1252  \n",
    "#### &nbsp;&nbsp;Corpus: 23023 sentence-level chunks  \n",
    "#### &nbsp;&nbsp;QRELS entries: 23023  \n",
    "#### &nbsp;&nbsp;Relevant entries: 6422 (27.9%)  \n",
    "#### &nbsp;&nbsp;Model: Qwen/Qwen2.5-0.5B-Instruct  \n",
    "================================================================================  \n",
    "\n",
    "The previously running experiment ragbench-covidqa-evaluation_1 was forcibly ended. Created a new experiment 'ragbench-covidqa-evaluation_2' with Experiment ID: 3 at /content/rapidfireai/rapidfire_experiments/ragbench-covidqa-evaluation_2  \n",
    "üåê Google Colab detected. Ray dashboard URL: https://8855-gpu-t4-hm-1uzxkq7hu4p2i-c.europe-west4-1.prod.colab.dev  \n",
    "üåê Google Colab detected. Dispatcher URL: https://8851-gpu-t4-hm-1uzxkq7hu4p2i-c.europe-west4-1.prod.colab.dev  \n",
    "üèÉ Running evaluation... (this may take several minutes)  \n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>\n",
    "\n",
    "### === Preprocessing RAG Sources ===\n",
    "\n",
    "| RAG Source ID | Status   | Duration | Details    |\n",
    "|---------------|---------|----------|------------|\n",
    "| 1             | Complete | 98.6s    | FAISS, GPU |\n",
    "\n",
    "---\n",
    "\n",
    "### === Multi-Config Experiment Progress ===\n",
    "\n",
    "| Run ID | Model                        | Status    | Progress | Conf. Interval | search_type | rag_k | top_n | chunk_size   | chunk_overlap | sampling_params                                      | model_config                                                                                                                         | Precision                    | Recall                       | MRR                          | Throughput | Total | Samples Processed | F1_Score                    | Hit_Rate                    | NDCG@k                     | Processing Time   | Samples Per Second | model_name                    | run_id |\n",
    "|--------|------------------------------|----------|---------|----------------|------------|-------|-------|--------------|---------------|-----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|-------------------------------|-------------------------------|------------|-------|------------------|-------------------------------|-------------------------------|-----------------------------|-----------------|-----------------|-------------------------------|--------|\n",
    "| 1      | Qwen/Qwen2.5-0.5B-Instruct  | COMPLETED | 4/4     | 0.000          | similarity | 10.00 | 5.00  | 100000.00    | 0.0000        | {'temperature': 0.7, 'top_p': 0.95, 'max_tokens': 256} | {'dtype': 'half', 'gpu_memory_utilization': 0.3, 'enforce_eager': True, 'max_model_len': 4096, 'disable_log_stats': True, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp'} | 11.93% [11.93%, 11.93%]     | 29.34% [29.34%, 29.34%]     | 39.02% [39.02%, 39.02%]     | 0.6/s      | 1,252 | 1,252           | 0.1545 [0.1545, 0.1545]      | 0.6526 [0.6526, 0.6526]      | 0.2622 [0.2622, 0.2622]    | 2206.19 seconds | 0.57            | Qwen/Qwen2.5-0.5B-Instruct  | 1.00   |\n",
    "\n",
    "**Note:** Experiment `ragbench-covidqa-evaluation_2` ended\n",
    "\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xyNZs2QUgl9"
   },
   "source": [
    "Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDA6tl6vbLeI",
    "outputId": "0effc87f-9470-4ff0-c819-3ebd3c1419b8"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESULTS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Results are automatically displayed by RapidFire\n",
    "# Additional analysis can be done here if needed\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline complete! Check the results above.\")\n",
    "print(f\"üìÅ Output files saved to: {OUTPUT_DIR}\")\n",
    "print(f\"   - corpus.jsonl\")\n",
    "print(f\"   - queries.csv\")\n",
    "print(f\"   - qrels.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ RAGBench Evaluation Pipeline Finished Successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìä RESULTS ANALYSIS\n",
    "================================================================================\n",
    "\n",
    "‚úÖ Pipeline complete! Check the results above.  \n",
    "üìÅ Output files saved to: `ragbench_output`  \n",
    "- `corpus.jsonl`  \n",
    "- `queries.csv`  \n",
    "- `qrels.csv`  \n",
    "\n",
    "---\n",
    "\n",
    "#### Metrics (Example from experiment)\n",
    "\n",
    "- **Precision**: 11.93%   \n",
    "- **Recall**: 29.34%  \n",
    "- **MRR**: 39.02%  \n",
    "- **F1 Score**: 0.1545   \n",
    "- **Hit Rate**: 0.6526  \n",
    "- **NDCG@k**: 0.2622 \n",
    "- **Throughput**: 0.6/s  \n",
    "- **Processing Time**: 2206.19 seconds  \n",
    "- **Samples Per Second**: 0.57  \n",
    "\n",
    "---\n",
    "\n",
    "#### üéâ RAGBench Evaluation Pipeline Finished Successfully!\n",
    "================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POrCdEs_mpoF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
